{
    "PySpark": [{
        "name": "SparkConf",
        "type": "class",
        "description": "PARAMETERS: loadDefaults=True, _jvm=None, _jconf=None\n             DESCRIPTION: Configuration for a Spark application. Used to set various Sparkundefinedparameters as key-value pairs.",
        "rightLabel": "class | PySpark",
        "displayText": "SparkConf(loadDefaults=True, _jvm=None, _jconf=None)",
        "text": "SparkConf()",
        "leftLabel": "SparkConf"
    }, {
        "name": "SparkConf.contains",
        "type": "method",
        "description": "PARAMETERS: key\n             DESCRIPTION: Does this configuration contain a given key?",
        "leftLabel": "SparkConf",
        "rightLabel": "method | PySpark",
        "displayText": "SparkConf.contains(key)",
        "text": "contains()"
    }, {
        "name": "SparkConf.get",
        "type": "method",
        "description": "PARAMETERS: key, defaultValue=None\n             DESCRIPTION: Get the configured value for some key, or return a default otherwise.",
        "leftLabel": "SparkConf",
        "rightLabel": "method | PySpark",
        "displayText": "SparkConf.get(key, defaultValue=None)",
        "text": "get()"
    }, {
        "name": "SparkConf.getAll",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Get all values as a list of key-value pairs.",
        "leftLabel": "SparkConf",
        "rightLabel": "method | PySpark",
        "displayText": "SparkConf.getAll()",
        "text": "getAll()"
    }, {
        "name": "SparkConf.set",
        "type": "method",
        "description": "PARAMETERS: key, value\n             DESCRIPTION: Set a configuration property.",
        "leftLabel": "SparkConf",
        "rightLabel": "method | PySpark",
        "displayText": "SparkConf.set(key, value)",
        "text": "set()"
    }, {
        "name": "SparkConf.setAll",
        "type": "method",
        "description": "PARAMETERS: pairs\n             DESCRIPTION: Set multiple parameters, passed as a list of key-value pairs.",
        "leftLabel": "SparkConf",
        "rightLabel": "method | PySpark",
        "displayText": "SparkConf.setAll(pairs)",
        "text": "setAll()"
    }, {
        "name": "SparkConf.setAppName",
        "type": "method",
        "description": "PARAMETERS: value\n             DESCRIPTION: Set application name.",
        "leftLabel": "SparkConf",
        "rightLabel": "method | PySpark",
        "displayText": "SparkConf.setAppName(value)",
        "text": "setAppName()"
    }, {
        "name": "SparkConf.setExecutorEnv",
        "type": "method",
        "description": "PARAMETERS: key=None, value=None, pairs=None\n             DESCRIPTION: Set an environment variable to be passed to executors.",
        "leftLabel": "SparkConf",
        "rightLabel": "method | PySpark",
        "displayText": "SparkConf.setExecutorEnv(key=None, value=None, pairs=None)",
        "text": "setExecutorEnv()"
    }, {
        "name": "SparkConf.setIfMissing",
        "type": "method",
        "description": "PARAMETERS: key, value\n             DESCRIPTION: Set a configuration property, if not already set.",
        "leftLabel": "SparkConf",
        "rightLabel": "method | PySpark",
        "displayText": "SparkConf.setIfMissing(key, value)",
        "text": "setIfMissing()"
    }, {
        "name": "SparkConf.setMaster",
        "type": "method",
        "description": "PARAMETERS: value\n             DESCRIPTION: Set master URL to connect to.",
        "leftLabel": "SparkConf",
        "rightLabel": "method | PySpark",
        "displayText": "SparkConf.setMaster(value)",
        "text": "setMaster()"
    }, {
        "name": "SparkConf.setSparkHome",
        "type": "method",
        "description": "PARAMETERS: value\n             DESCRIPTION: Set path where Spark is installed on worker nodes.",
        "leftLabel": "SparkConf",
        "rightLabel": "method | PySpark",
        "displayText": "SparkConf.setSparkHome(value)",
        "text": "setSparkHome()"
    }, {
        "name": "SparkConf.toDebugString",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Returns a printable version of the configuration, as a list ofundefinedkey=value pairs, one per line.",
        "leftLabel": "SparkConf",
        "rightLabel": "method | PySpark",
        "displayText": "SparkConf.toDebugString()",
        "text": "toDebugString()"
    }, {
        "name": "SparkContext",
        "type": "class",
        "description": "PARAMETERS: master=None, appName=None, sparkHome=None, pyFiles=None, environment=None, batchSize=0, serializer=PickleSerializer(), conf=None, gateway=None, jsc=None, profiler_cls=&lt;class 'pyspark.profiler.BasicProfiler'&gt;\n             DESCRIPTION: Main entry point for Spark functionality. A SparkContext represents theundefinedconnection to a Spark cluster, and can be used to create RDD and\nbroadcast variables on that cluster.",
        "rightLabel": "class | PySpark",
        "displayText": "SparkContext(master=None, appName=None, sparkHome=None, pyFiles=None, environment=None, batchSize=0, serializer=PickleSerializer(), conf=None, gateway=None, jsc=None, profiler_cls=&lt;class 'pyspark.profiler.BasicProfiler'&gt;)",
        "text": "SparkContext()",
        "leftLabel": "SparkContext"
    }, {
        "name": "SparkContext.PACKAGE_EXTENSIONS",
        "type": "value",
        "description": "PARAMETERS: value, accum_param=None\n             DESCRIPTION: Create an Accumulator with the given initial value, using a givenundefinedAccumulatorParam helper object to define how to add values of the\ndata type if provided. Default AccumulatorParams are used for integers\nand floating-point numbers if you do not provide one. For other types,\na custom AccumulatorParam can be used.",
        "leftLabel": "SparkContext",
        "rightLabel": "attribute | PySpark",
        "displayText": "SparkContext.PACKAGE_EXTENSIONS",
        "text": "PACKAGE_EXTENSIONS"
    }, {
        "name": "SparkContext.applicationId",
        "type": "value",
        "description": "PARAMETERS: path, minPartitions=None\n             DESCRIPTION: A unique identifier for the Spark application.undefinedIts format depends on the scheduler implementation.",
        "leftLabel": "SparkContext",
        "rightLabel": "attribute | PySpark",
        "displayText": "SparkContext.applicationId",
        "text": "applicationId"
    }, {
        "name": "SparkContext.defaultMinPartitions",
        "type": "value",
        "description": "PARAMETERS: \n             DESCRIPTION: Default min number of partitions for Hadoop RDDs when not given by user",
        "leftLabel": "SparkContext",
        "rightLabel": "attribute | PySpark",
        "displayText": "SparkContext.defaultMinPartitions",
        "text": "defaultMinPartitions"
    }, {
        "name": "SparkContext.defaultParallelism",
        "type": "value",
        "description": "PARAMETERS: path\n             DESCRIPTION: Default level of parallelism to use when not given by user (e.g. forundefinedreduce tasks)",
        "leftLabel": "SparkContext",
        "rightLabel": "attribute | PySpark",
        "displayText": "SparkContext.defaultParallelism",
        "text": "defaultParallelism"
    }, {
        "name": "SparkContext.startTime",
        "type": "value",
        "description": "PARAMETERS: \n             DESCRIPTION: Return the epoch time when the Spark Context was started.",
        "leftLabel": "SparkContext",
        "rightLabel": "attribute | PySpark",
        "displayText": "SparkContext.startTime",
        "text": "startTime"
    }, {
        "name": "SparkContext.version",
        "type": "value",
        "description": "PARAMETERS: path, minPartitions=None, use_unicode=True\n             DESCRIPTION: The version of Spark on which this application is running.",
        "leftLabel": "SparkContext",
        "rightLabel": "attribute | PySpark",
        "displayText": "SparkContext.version",
        "text": "version"
    }, {
        "name": "SparkContext.accumulator",
        "type": "method",
        "description": "PARAMETERS: value, accum_param=None\n             DESCRIPTION: Create an Accumulator with the given initial value, using a givenundefinedAccumulatorParam helper object to define how to add values of the\ndata type if provided. Default AccumulatorParams are used for integers\nand floating-point numbers if you do not provide one. For other types,\na custom AccumulatorParam can be used.",
        "leftLabel": "SparkContext",
        "rightLabel": "method | PySpark",
        "displayText": "SparkContext.accumulator(value, accum_param=None)",
        "text": "accumulator()"
    }, {
        "name": "SparkContext.addFile",
        "type": "method",
        "description": "PARAMETERS: path\n             DESCRIPTION: Add a file to be downloaded with this Spark job on every node.undefinedThe path passed can be either a local file, a file in HDFS\n(or other Hadoop-supported filesystems), or an HTTP, HTTPS or\nFTP URI.",
        "leftLabel": "SparkContext",
        "rightLabel": "method | PySpark",
        "displayText": "SparkContext.addFile(path)",
        "text": "addFile()"
    }, {
        "name": "SparkContext.addPyFile",
        "type": "method",
        "description": "PARAMETERS: path\n             DESCRIPTION: Add a .py or .zip dependency for all tasks to be executed on thisundefinedSparkContext in the future.  The path passed can be either a local\nfile, a file in HDFS (or other Hadoop-supported filesystems), or an\nHTTP, HTTPS or FTP URI.",
        "leftLabel": "SparkContext",
        "rightLabel": "method | PySpark",
        "displayText": "SparkContext.addPyFile(path)",
        "text": "addPyFile()"
    }, {
        "name": "SparkContext.binaryFiles",
        "type": "method",
        "description": "PARAMETERS: path, minPartitions=None\n             DESCRIPTION: Read a directory of binary files from HDFS, a local file systemundefined(available on all nodes), or any Hadoop-supported file system URI\nas a byte array. Each file is read as a single record and returned\nin a key-value pair, where the key is the path of each file, the\nvalue is the content of each file.",
        "leftLabel": "SparkContext",
        "rightLabel": "method | PySpark",
        "displayText": "SparkContext.binaryFiles(path, minPartitions=None)",
        "text": "binaryFiles()"
    }, {
        "name": "SparkContext.binaryRecords",
        "type": "method",
        "description": "PARAMETERS: path, recordLength\n             DESCRIPTION: Load data from a flat binary file, assuming each record is a set of numbersundefinedwith the specified numerical format (see ByteBuffer), and the number of\nbytes per record is constant.",
        "leftLabel": "SparkContext",
        "rightLabel": "method | PySpark",
        "displayText": "SparkContext.binaryRecords(path, recordLength)",
        "text": "binaryRecords()"
    }, {
        "name": "SparkContext.broadcast",
        "type": "method",
        "description": "PARAMETERS: value\n             DESCRIPTION: Broadcast a read-only variable to the cluster, returning aundefinedL{Broadcast&lt;pyspark.broadcast.Broadcast&gt;}\nobject for reading it in distributed functions. The variable will\nbe sent to each cluster only once.",
        "leftLabel": "SparkContext",
        "rightLabel": "method | PySpark",
        "displayText": "SparkContext.broadcast(value)",
        "text": "broadcast()"
    }, {
        "name": "SparkContext.cancelAllJobs",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Cancel all jobs that have been scheduled or are running.",
        "leftLabel": "SparkContext",
        "rightLabel": "method | PySpark",
        "displayText": "SparkContext.cancelAllJobs()",
        "text": "cancelAllJobs()"
    }, {
        "name": "SparkContext.cancelJobGroup",
        "type": "method",
        "description": "PARAMETERS: groupId\n             DESCRIPTION: Cancel active jobs for the specified group. See SparkContext.setJobGroupundefinedfor more information.",
        "leftLabel": "SparkContext",
        "rightLabel": "method | PySpark",
        "displayText": "SparkContext.cancelJobGroup(groupId)",
        "text": "cancelJobGroup()"
    }, {
        "name": "SparkContext.clearFiles",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Clear the job&#8217;s list of files added by addFile or addPyFile soundefinedthat they do not get downloaded to any new nodes.",
        "leftLabel": "SparkContext",
        "rightLabel": "method | PySpark",
        "displayText": "SparkContext.clearFiles()",
        "text": "clearFiles()"
    }, {
        "name": "SparkContext.dump_profiles",
        "type": "method",
        "description": "PARAMETERS: path\n             DESCRIPTION: Dump the profile stats into directory path",
        "leftLabel": "SparkContext",
        "rightLabel": "method | PySpark",
        "displayText": "SparkContext.dump_profiles(path)",
        "text": "dump_profiles()"
    }, {
        "name": "SparkContext.emptyRDD",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Create an RDD that has no partitions or elements.",
        "leftLabel": "SparkContext",
        "rightLabel": "method | PySpark",
        "displayText": "SparkContext.emptyRDD()",
        "text": "emptyRDD()"
    }, {
        "name": "SparkContext.getLocalProperty",
        "type": "method",
        "description": "PARAMETERS: key\n             DESCRIPTION: Get a local property set in this thread, or null if it is missing. SeeundefinedsetLocalProperty",
        "leftLabel": "SparkContext",
        "rightLabel": "method | PySpark",
        "displayText": "SparkContext.getLocalProperty(key)",
        "text": "getLocalProperty()"
    }, {
        "name": "SparkContext.hadoopFile",
        "type": "method",
        "description": "PARAMETERS: path, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0\n             DESCRIPTION: Read an &#8216;old&#8217; Hadoop InputFormat with arbitrary key and value class from HDFS,undefineda local file system (available on all nodes), or any Hadoop-supported file system URI.\nThe mechanism is the same as for sc.sequenceFile.",
        "leftLabel": "SparkContext",
        "rightLabel": "method | PySpark",
        "displayText": "SparkContext.hadoopFile(path, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)",
        "text": "hadoopFile()"
    }, {
        "name": "SparkContext.hadoopRDD",
        "type": "method",
        "description": "PARAMETERS: inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0\n             DESCRIPTION: Read an &#8216;old&#8217; Hadoop InputFormat with arbitrary key and value class, from an arbitraryundefinedHadoop configuration, which is passed in as a Python dict.\nThis will be converted into a Configuration in Java.\nThe mechanism is the same as for sc.sequenceFile.",
        "leftLabel": "SparkContext",
        "rightLabel": "method | PySpark",
        "displayText": "SparkContext.hadoopRDD(inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)",
        "text": "hadoopRDD()"
    }, {
        "name": "SparkContext.newAPIHadoopFile",
        "type": "method",
        "description": "PARAMETERS: path, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0\n             DESCRIPTION: Read a &#8216;new API&#8217; Hadoop InputFormat with arbitrary key and value class from HDFS,undefineda local file system (available on all nodes), or any Hadoop-supported file system URI.\nThe mechanism is the same as for sc.sequenceFile.",
        "leftLabel": "SparkContext",
        "rightLabel": "method | PySpark",
        "displayText": "SparkContext.newAPIHadoopFile(path, inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)",
        "text": "newAPIHadoopFile()"
    }, {
        "name": "SparkContext.newAPIHadoopRDD",
        "type": "method",
        "description": "PARAMETERS: inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0\n             DESCRIPTION: Read a &#8216;new API&#8217; Hadoop InputFormat with arbitrary key and value class, from an arbitraryundefinedHadoop configuration, which is passed in as a Python dict.\nThis will be converted into a Configuration in Java.\nThe mechanism is the same as for sc.sequenceFile.",
        "leftLabel": "SparkContext",
        "rightLabel": "method | PySpark",
        "displayText": "SparkContext.newAPIHadoopRDD(inputFormatClass, keyClass, valueClass, keyConverter=None, valueConverter=None, conf=None, batchSize=0)",
        "text": "newAPIHadoopRDD()"
    }, {
        "name": "SparkContext.parallelize",
        "type": "method",
        "description": "PARAMETERS: c, numSlices=None\n             DESCRIPTION: Distribute a local Python collection to form an RDD. Using xrangeundefinedis recommended if the input represents a range for performance.",
        "leftLabel": "SparkContext",
        "rightLabel": "method | PySpark",
        "displayText": "SparkContext.parallelize(c, numSlices=None)",
        "text": "parallelize()"
    }, {
        "name": "SparkContext.pickleFile",
        "type": "method",
        "description": "PARAMETERS: name, minPartitions=None\n             DESCRIPTION: Load an RDD previously saved using RDD.saveAsPickleFile method.",
        "leftLabel": "SparkContext",
        "rightLabel": "method | PySpark",
        "displayText": "SparkContext.pickleFile(name, minPartitions=None)",
        "text": "pickleFile()"
    }, {
        "name": "SparkContext.range",
        "type": "method",
        "description": "PARAMETERS: start, end=None, step=1, numSlices=None\n             DESCRIPTION: Create a new RDD of int containing elements from start to endundefined(exclusive), increased by step every element. Can be called the same\nway as python&#8217;s built-in range() function. If called with a single argument,\nthe argument is interpreted as end, and start is set to 0.",
        "leftLabel": "SparkContext",
        "rightLabel": "method | PySpark",
        "displayText": "SparkContext.range(start, end=None, step=1, numSlices=None)",
        "text": "range()"
    }, {
        "name": "SparkContext.runJob",
        "type": "method",
        "description": "PARAMETERS: rdd, partitionFunc, partitions=None, allowLocal=False\n             DESCRIPTION: Executes the given partitionFunc on the specified set of partitions,undefinedreturning the result as an array of elements.",
        "leftLabel": "SparkContext",
        "rightLabel": "method | PySpark",
        "displayText": "SparkContext.runJob(rdd, partitionFunc, partitions=None, allowLocal=False)",
        "text": "runJob()"
    }, {
        "name": "SparkContext.sequenceFile",
        "type": "method",
        "description": "PARAMETERS: path, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, minSplits=None, batchSize=0\n             DESCRIPTION: Read a Hadoop SequenceFile with arbitrary key and value Writable class from HDFS,undefineda local file system (available on all nodes), or any Hadoop-supported file system URI.\nThe mechanism is as follows:",
        "leftLabel": "SparkContext",
        "rightLabel": "method | PySpark",
        "displayText": "SparkContext.sequenceFile(path, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, minSplits=None, batchSize=0)",
        "text": "sequenceFile()"
    }, {
        "name": "SparkContext.setCheckpointDir",
        "type": "method",
        "description": "PARAMETERS: dirName\n             DESCRIPTION: Set the directory under which RDDs are going to be checkpointed. Theundefineddirectory must be a HDFS path if running on a cluster.",
        "leftLabel": "SparkContext",
        "rightLabel": "method | PySpark",
        "displayText": "SparkContext.setCheckpointDir(dirName)",
        "text": "setCheckpointDir()"
    }, {
        "name": "SparkContext.setJobGroup",
        "type": "method",
        "description": "PARAMETERS: groupId, description, interruptOnCancel=False\n             DESCRIPTION: Assigns a group ID to all the jobs started by this thread until the group ID is set to aundefineddifferent value or cleared.",
        "leftLabel": "SparkContext",
        "rightLabel": "method | PySpark",
        "displayText": "SparkContext.setJobGroup(groupId, description, interruptOnCancel=False)",
        "text": "setJobGroup()"
    }, {
        "name": "SparkContext.setLocalProperty",
        "type": "method",
        "description": "PARAMETERS: key, value\n             DESCRIPTION: Set a local property that affects jobs submitted from this thread, such as theundefinedSpark fair scheduler pool.",
        "leftLabel": "SparkContext",
        "rightLabel": "method | PySpark",
        "displayText": "SparkContext.setLocalProperty(key, value)",
        "text": "setLocalProperty()"
    }, {
        "name": "SparkContext.setLogLevel",
        "type": "method",
        "description": "PARAMETERS: logLevel\n             DESCRIPTION: Control our logLevel. This overrides any user-defined log settings.undefinedValid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN",
        "leftLabel": "SparkContext",
        "rightLabel": "method | PySpark",
        "displayText": "SparkContext.setLogLevel(logLevel)",
        "text": "setLogLevel()"
    }, {
        "name": "SparkContext.show_profiles",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Print the profile stats to stdout",
        "leftLabel": "SparkContext",
        "rightLabel": "method | PySpark",
        "displayText": "SparkContext.show_profiles()",
        "text": "show_profiles()"
    }, {
        "name": "SparkContext.sparkUser",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Get SPARK_USER for user who is running SparkContext.",
        "leftLabel": "SparkContext",
        "rightLabel": "method | PySpark",
        "displayText": "SparkContext.sparkUser()",
        "text": "sparkUser()"
    }, {
        "name": "SparkContext.statusTracker",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Return StatusTracker object",
        "leftLabel": "SparkContext",
        "rightLabel": "method | PySpark",
        "displayText": "SparkContext.statusTracker()",
        "text": "statusTracker()"
    }, {
        "name": "SparkContext.stop",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Shut down the SparkContext.",
        "leftLabel": "SparkContext",
        "rightLabel": "method | PySpark",
        "displayText": "SparkContext.stop()",
        "text": "stop()"
    }, {
        "name": "SparkContext.textFile",
        "type": "method",
        "description": "PARAMETERS: name, minPartitions=None, use_unicode=True\n             DESCRIPTION: Read a text file from HDFS, a local file system (available on allundefinednodes), or any Hadoop-supported file system URI, and return it as an\nRDD of Strings.",
        "leftLabel": "SparkContext",
        "rightLabel": "method | PySpark",
        "displayText": "SparkContext.textFile(name, minPartitions=None, use_unicode=True)",
        "text": "textFile()"
    }, {
        "name": "SparkContext.union",
        "type": "method",
        "description": "PARAMETERS: rdds\n             DESCRIPTION: Build the union of a list of RDDs.",
        "leftLabel": "SparkContext",
        "rightLabel": "method | PySpark",
        "displayText": "SparkContext.union(rdds)",
        "text": "union()"
    }, {
        "name": "SparkContext.wholeTextFiles",
        "type": "method",
        "description": "PARAMETERS: path, minPartitions=None, use_unicode=True\n             DESCRIPTION: Read a directory of text files from HDFS, a local file systemundefined(available on all nodes), or any  Hadoop-supported file system\nURI. Each file is read as a single record and returned in a\nkey-value pair, where the key is the path of each file, the\nvalue is the content of each file.",
        "leftLabel": "SparkContext",
        "rightLabel": "method | PySpark",
        "displayText": "SparkContext.wholeTextFiles(path, minPartitions=None, use_unicode=True)",
        "text": "wholeTextFiles()"
    }, {
        "name": "SparkFiles",
        "type": "class",
        "description": "PARAMETERS: filename\n             DESCRIPTION: Resolves paths to files added throughundefinedL{SparkContext.addFile()&lt;pyspark.context.SparkContext.addFile&gt;}.",
        "rightLabel": "class | PySpark",
        "displayText": "SparkFiles(filename)",
        "text": "SparkFiles()",
        "leftLabel": "SparkFiles"
    }, {
        "name": "RDD",
        "type": "class",
        "description": "PARAMETERS: jrdd, ctx, jrdd_deserializer=AutoBatchedSerializer(PickleSerializer())\n             DESCRIPTION: A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.undefinedRepresents an immutable, partitioned collection of elements that can be\noperated on in parallel.",
        "rightLabel": "class | PySpark",
        "displayText": "RDD(jrdd, ctx, jrdd_deserializer=AutoBatchedSerializer(PickleSerializer()))",
        "text": "RDD()",
        "leftLabel": "RDD"
    }, {
        "name": "RDD.context",
        "type": "value",
        "description": "PARAMETERS: \n             DESCRIPTION: The SparkContext that this RDD was created on.",
        "leftLabel": "RDD",
        "rightLabel": "attribute | PySpark",
        "displayText": "RDD.context",
        "text": "context"
    }, {
        "name": "RDD.aggregate",
        "type": "method",
        "description": "PARAMETERS: zeroValue, seqOp, combOp\n             DESCRIPTION: Aggregate the elements of each partition, and then the results for allundefinedthe partitions, using a given combine functions and a neutral &#8220;zero\nvalue.&#8221;",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.aggregate(zeroValue, seqOp, combOp)",
        "text": "aggregate()"
    }, {
        "name": "RDD.aggregateByKey",
        "type": "method",
        "description": "PARAMETERS: zeroValue, seqFunc, combFunc, numPartitions=None, partitionFunc=&lt;function portable_hash at 0x7f1ac7340578&gt;\n             DESCRIPTION: Aggregate the values of each key, using given combine functions and a neutralundefined&#8220;zero value&#8221;. This function can return a different result type, U, than the type\nof the values in this RDD, V. Thus, we need one operation for merging a V into\na U and one operation for merging two U&#8217;s, The former operation is used for merging\nvalues within a partition, and the latter is used for merging values between\npartitions. To avoid memory allocation, both of these functions are\nallowed to modify and return their first argument instead of creating a new U.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.aggregateByKey(zeroValue, seqFunc, combFunc, numPartitions=None, partitionFunc=&lt;function portable_hash at 0x7f1ac7340578&gt;)",
        "text": "aggregateByKey()"
    }, {
        "name": "RDD.cache",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Persist this RDD with the default storage level (MEMORY_ONLY_SER).",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.cache()",
        "text": "cache()"
    }, {
        "name": "RDD.cartesian",
        "type": "method",
        "description": "PARAMETERS: other\n             DESCRIPTION: Return the Cartesian product of this RDD and another one, that is, theundefinedRDD of all pairs of elements (a, b) where a is in self and\nb is in other.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.cartesian(other)",
        "text": "cartesian()"
    }, {
        "name": "RDD.checkpoint",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Mark this RDD for checkpointing. It will be saved to a file inside theundefinedcheckpoint directory set with SparkContext.setCheckpointDir() and\nall references to its parent RDDs will be removed. This function must\nbe called before any job has been executed on this RDD. It is strongly\nrecommended that this RDD is persisted in memory, otherwise saving it\non a file will require recomputation.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.checkpoint()",
        "text": "checkpoint()"
    }, {
        "name": "RDD.coalesce",
        "type": "method",
        "description": "PARAMETERS: numPartitions, shuffle=False\n             DESCRIPTION: Return a new RDD that is reduced into numPartitions partitions.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.coalesce(numPartitions, shuffle=False)",
        "text": "coalesce()"
    }, {
        "name": "RDD.cogroup",
        "type": "method",
        "description": "PARAMETERS: other, numPartitions=None\n             DESCRIPTION: For each key k in self or other, return a resulting RDD thatundefinedcontains a tuple with the list of values for that key in self as\nwell as other.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.cogroup(other, numPartitions=None)",
        "text": "cogroup()"
    }, {
        "name": "RDD.collect",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Return a list that contains all of the elements in this RDD.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.collect()",
        "text": "collect()"
    }, {
        "name": "RDD.collectAsMap",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Return the key-value pairs in this RDD to the master as a dictionary.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.collectAsMap()",
        "text": "collectAsMap()"
    }, {
        "name": "RDD.combineByKey",
        "type": "method",
        "description": "PARAMETERS: createCombiner, mergeValue, mergeCombiners, numPartitions=None, partitionFunc=&lt;function portable_hash at 0x7f1ac7340578&gt;\n             DESCRIPTION: Generic function to combine the elements for each key using a customundefinedset of aggregation functions.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.combineByKey(createCombiner, mergeValue, mergeCombiners, numPartitions=None, partitionFunc=&lt;function portable_hash at 0x7f1ac7340578&gt;)",
        "text": "combineByKey()"
    }, {
        "name": "RDD.count",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Return the number of elements in this RDD.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.count()",
        "text": "count()"
    }, {
        "name": "RDD.countApprox",
        "type": "method",
        "description": "PARAMETERS: timeout, confidence=0.95\n             DESCRIPTION: Approximate version of count() that returns a potentially incompleteundefinedresult within a timeout, even if not all tasks have finished.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.countApprox(timeout, confidence=0.95)",
        "text": "countApprox()"
    }, {
        "name": "RDD.countApproxDistinct",
        "type": "method",
        "description": "PARAMETERS: relativeSD=0.05\n             DESCRIPTION: Return approximate number of distinct elements in the RDD.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.countApproxDistinct(relativeSD=0.05)",
        "text": "countApproxDistinct()"
    }, {
        "name": "RDD.countByKey",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Count the number of elements for each key, and return the result to theundefinedmaster as a dictionary.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.countByKey()",
        "text": "countByKey()"
    }, {
        "name": "RDD.countByValue",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Return the count of each unique value in this RDD as a dictionary ofundefined(value, count) pairs.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.countByValue()",
        "text": "countByValue()"
    }, {
        "name": "RDD.distinct",
        "type": "method",
        "description": "PARAMETERS: numPartitions=None\n             DESCRIPTION: Return a new RDD containing the distinct elements in this RDD.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.distinct(numPartitions=None)",
        "text": "distinct()"
    }, {
        "name": "RDD.filter",
        "type": "method",
        "description": "PARAMETERS: f\n             DESCRIPTION: Return a new RDD containing only the elements that satisfy a predicate.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.filter(f)",
        "text": "filter()"
    }, {
        "name": "RDD.first",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Return the first element in this RDD.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.first()",
        "text": "first()"
    }, {
        "name": "RDD.flatMap",
        "type": "method",
        "description": "PARAMETERS: f, preservesPartitioning=False\n             DESCRIPTION: Return a new RDD by first applying a function to all elements of thisundefinedRDD, and then flattening the results.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.flatMap(f, preservesPartitioning=False)",
        "text": "flatMap()"
    }, {
        "name": "RDD.flatMapValues",
        "type": "method",
        "description": "PARAMETERS: f\n             DESCRIPTION: Pass each value in the key-value pair RDD through a flatMap functionundefinedwithout changing the keys; this also retains the original RDD&#8217;s\npartitioning.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.flatMapValues(f)",
        "text": "flatMapValues()"
    }, {
        "name": "RDD.fold",
        "type": "method",
        "description": "PARAMETERS: zeroValue, op\n             DESCRIPTION: Aggregate the elements of each partition, and then the results for allundefinedthe partitions, using a given associative and commutative function and\na neutral &#8220;zero value.&#8221;",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.fold(zeroValue, op)",
        "text": "fold()"
    }, {
        "name": "RDD.foldByKey",
        "type": "method",
        "description": "PARAMETERS: zeroValue, func, numPartitions=None, partitionFunc=&lt;function portable_hash at 0x7f1ac7340578&gt;\n             DESCRIPTION: Merge the values for each key using an associative function &#8220;func&#8221;undefinedand a neutral &#8220;zeroValue&#8221; which may be added to the result an\narbitrary number of times, and must not change the result\n(e.g., 0 for addition, or 1 for multiplication.).",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.foldByKey(zeroValue, func, numPartitions=None, partitionFunc=&lt;function portable_hash at 0x7f1ac7340578&gt;)",
        "text": "foldByKey()"
    }, {
        "name": "RDD.foreach",
        "type": "method",
        "description": "PARAMETERS: f\n             DESCRIPTION: Applies a function to all elements of this RDD.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.foreach(f)",
        "text": "foreach()"
    }, {
        "name": "RDD.foreachPartition",
        "type": "method",
        "description": "PARAMETERS: f\n             DESCRIPTION: Applies a function to each partition of this RDD.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.foreachPartition(f)",
        "text": "foreachPartition()"
    }, {
        "name": "RDD.fullOuterJoin",
        "type": "method",
        "description": "PARAMETERS: other, numPartitions=None\n             DESCRIPTION: Perform a right outer join of self and other.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.fullOuterJoin(other, numPartitions=None)",
        "text": "fullOuterJoin()"
    }, {
        "name": "RDD.getCheckpointFile",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Gets the name of the file to which this RDD was checkpointed",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.getCheckpointFile()",
        "text": "getCheckpointFile()"
    }, {
        "name": "RDD.getNumPartitions",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Returns the number of partitions in RDD",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.getNumPartitions()",
        "text": "getNumPartitions()"
    }, {
        "name": "RDD.getStorageLevel",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Get the RDD&#8217;s current storage level.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.getStorageLevel()",
        "text": "getStorageLevel()"
    }, {
        "name": "RDD.glom",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Return an RDD created by coalescing all elements within each partitionundefinedinto a list.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.glom()",
        "text": "glom()"
    }, {
        "name": "RDD.groupBy",
        "type": "method",
        "description": "PARAMETERS: f, numPartitions=None, partitionFunc=&lt;function portable_hash at 0x7f1ac7340578&gt;\n             DESCRIPTION: Return an RDD of grouped items.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.groupBy(f, numPartitions=None, partitionFunc=&lt;function portable_hash at 0x7f1ac7340578&gt;)",
        "text": "groupBy()"
    }, {
        "name": "RDD.groupByKey",
        "type": "method",
        "description": "PARAMETERS: numPartitions=None, partitionFunc=&lt;function portable_hash at 0x7f1ac7340578&gt;\n             DESCRIPTION: Group the values for each key in the RDD into a single sequence.undefinedHash-partitions the resulting RDD with numPartitions partitions.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.groupByKey(numPartitions=None, partitionFunc=&lt;function portable_hash at 0x7f1ac7340578&gt;)",
        "text": "groupByKey()"
    }, {
        "name": "RDD.groupWith",
        "type": "method",
        "description": "PARAMETERS: other, *others\n             DESCRIPTION: Alias for cogroup but with support for multiple RDDs.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.groupWith(other, *others)",
        "text": "groupWith()"
    }, {
        "name": "RDD.histogram",
        "type": "method",
        "description": "PARAMETERS: buckets\n             DESCRIPTION: Compute a histogram using the provided buckets. The bucketsundefinedare all open to the right except for the last which is closed.\ne.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],\nwhich means 1&lt;=x&lt;10, 10&lt;=x&lt;20, 20&lt;=x&lt;=50. And on the input of 1\nand 50 we would have a histogram of 1,0,1.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.histogram(buckets)",
        "text": "histogram()"
    }, {
        "name": "RDD.id",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: A unique ID for this RDD (within its SparkContext).",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.id()",
        "text": "id()"
    }, {
        "name": "RDD.intersection",
        "type": "method",
        "description": "PARAMETERS: other\n             DESCRIPTION: Return the intersection of this RDD and another one. The output willundefinednot contain any duplicate elements, even if the input RDDs did.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.intersection(other)",
        "text": "intersection()"
    }, {
        "name": "RDD.isCheckpointed",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Return whether this RDD has been checkpointed or not",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.isCheckpointed()",
        "text": "isCheckpointed()"
    }, {
        "name": "RDD.isEmpty",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Returns true if and only if the RDD contains no elements at all. Note that an RDDundefinedmay be empty even when it has at least 1 partition.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.isEmpty()",
        "text": "isEmpty()"
    }, {
        "name": "RDD.join",
        "type": "method",
        "description": "PARAMETERS: other, numPartitions=None\n             DESCRIPTION: Return an RDD containing all pairs of elements with matching keys inundefinedself and other.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.join(other, numPartitions=None)",
        "text": "join()"
    }, {
        "name": "RDD.keyBy",
        "type": "method",
        "description": "PARAMETERS: f\n             DESCRIPTION: Creates tuples of the elements in this RDD by applying f.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.keyBy(f)",
        "text": "keyBy()"
    }, {
        "name": "RDD.keys",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Return an RDD with the keys of each tuple.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.keys()",
        "text": "keys()"
    }, {
        "name": "RDD.leftOuterJoin",
        "type": "method",
        "description": "PARAMETERS: other, numPartitions=None\n             DESCRIPTION: Perform a left outer join of self and other.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.leftOuterJoin(other, numPartitions=None)",
        "text": "leftOuterJoin()"
    }, {
        "name": "RDD.lookup",
        "type": "method",
        "description": "PARAMETERS: key\n             DESCRIPTION: Return the list of values in the RDD for key key. This operationundefinedis done efficiently if the RDD has a known partitioner by only\nsearching the partition that the key maps to.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.lookup(key)",
        "text": "lookup()"
    }, {
        "name": "RDD.map",
        "type": "method",
        "description": "PARAMETERS: f, preservesPartitioning=False\n             DESCRIPTION: Return a new RDD by applying a function to each element of this RDD.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.map(f, preservesPartitioning=False)",
        "text": "map()"
    }, {
        "name": "RDD.mapPartitions",
        "type": "method",
        "description": "PARAMETERS: f, preservesPartitioning=False\n             DESCRIPTION: Return a new RDD by applying a function to each partition of this RDD.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.mapPartitions(f, preservesPartitioning=False)",
        "text": "mapPartitions()"
    }, {
        "name": "RDD.mapPartitionsWithIndex",
        "type": "method",
        "description": "PARAMETERS: f, preservesPartitioning=False\n             DESCRIPTION: Return a new RDD by applying a function to each partition of this RDD,undefinedwhile tracking the index of the original partition.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.mapPartitionsWithIndex(f, preservesPartitioning=False)",
        "text": "mapPartitionsWithIndex()"
    }, {
        "name": "RDD.mapPartitionsWithSplit",
        "type": "method",
        "description": "PARAMETERS: f, preservesPartitioning=False\n             DESCRIPTION: Deprecated: use mapPartitionsWithIndex instead.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.mapPartitionsWithSplit(f, preservesPartitioning=False)",
        "text": "mapPartitionsWithSplit()"
    }, {
        "name": "RDD.mapValues",
        "type": "method",
        "description": "PARAMETERS: f\n             DESCRIPTION: Pass each value in the key-value pair RDD through a map functionundefinedwithout changing the keys; this also retains the original RDD&#8217;s\npartitioning.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.mapValues(f)",
        "text": "mapValues()"
    }, {
        "name": "RDD.max",
        "type": "method",
        "description": "PARAMETERS: key=None\n             DESCRIPTION: Find the maximum item in this RDD.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.max(key=None)",
        "text": "max()"
    }, {
        "name": "RDD.mean",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Compute the mean of this RDD&#8217;s elements.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.mean()",
        "text": "mean()"
    }, {
        "name": "RDD.meanApprox",
        "type": "method",
        "description": "PARAMETERS: timeout, confidence=0.95\n             DESCRIPTION: Approximate operation to return the mean within a timeoutundefinedor meet the confidence.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.meanApprox(timeout, confidence=0.95)",
        "text": "meanApprox()"
    }, {
        "name": "RDD.min",
        "type": "method",
        "description": "PARAMETERS: key=None\n             DESCRIPTION: Find the minimum item in this RDD.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.min(key=None)",
        "text": "min()"
    }, {
        "name": "RDD.name",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Return the name of this RDD.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.name()",
        "text": "name()"
    }, {
        "name": "RDD.partitionBy",
        "type": "method",
        "description": "PARAMETERS: numPartitions, partitionFunc=&lt;function portable_hash at 0x7f1ac7340578&gt;\n             DESCRIPTION: Return a copy of the RDD partitioned using the specified partitioner.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.partitionBy(numPartitions, partitionFunc=&lt;function portable_hash at 0x7f1ac7340578&gt;)",
        "text": "partitionBy()"
    }, {
        "name": "RDD.persist",
        "type": "method",
        "description": "PARAMETERS: storageLevel=StorageLevel(False, True, False, False, 1)\n             DESCRIPTION: Set this RDD&#8217;s storage level to persist its values across operationsundefinedafter the first time it is computed. This can only be used to assign\na new storage level if the RDD does not have a storage level set yet.\nIf no storage level is specified defaults to (MEMORY_ONLY_SER).",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.persist(storageLevel=StorageLevel(False, True, False, False, 1))",
        "text": "persist()"
    }, {
        "name": "RDD.pipe",
        "type": "method",
        "description": "PARAMETERS: command, env=None, checkCode=False\n             DESCRIPTION: Return an RDD created by piping elements to a forked external process.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.pipe(command, env=None, checkCode=False)",
        "text": "pipe()"
    }, {
        "name": "RDD.randomSplit",
        "type": "method",
        "description": "PARAMETERS: weights, seed=None\n             DESCRIPTION: Randomly splits this RDD with the provided weights.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.randomSplit(weights, seed=None)",
        "text": "randomSplit()"
    }, {
        "name": "RDD.reduce",
        "type": "method",
        "description": "PARAMETERS: f\n             DESCRIPTION: Reduces the elements of this RDD using the specified commutative andundefinedassociative binary operator. Currently reduces partitions locally.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.reduce(f)",
        "text": "reduce()"
    }, {
        "name": "RDD.reduceByKey",
        "type": "method",
        "description": "PARAMETERS: func, numPartitions=None, partitionFunc=&lt;function portable_hash at 0x7f1ac7340578&gt;\n             DESCRIPTION: Merge the values for each key using an associative reduce function.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.reduceByKey(func, numPartitions=None, partitionFunc=&lt;function portable_hash at 0x7f1ac7340578&gt;)",
        "text": "reduceByKey()"
    }, {
        "name": "RDD.reduceByKeyLocally",
        "type": "method",
        "description": "PARAMETERS: func\n             DESCRIPTION: Merge the values for each key using an associative reduce function, butundefinedreturn the results immediately to the master as a dictionary.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.reduceByKeyLocally(func)",
        "text": "reduceByKeyLocally()"
    }, {
        "name": "RDD.repartition",
        "type": "method",
        "description": "PARAMETERS: numPartitions\n             DESCRIPTION: Return a new RDD that has exactly numPartitions partitions.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.repartition(numPartitions)",
        "text": "repartition()"
    }, {
        "name": "RDD.repartitionAndSortWithinPartitions",
        "type": "method",
        "description": "PARAMETERS: numPartitions=None, partitionFunc=&lt;function portable_hash at 0x7f1ac7340578&gt;, ascending=True, keyfunc=&lt;function &lt;lambda&gt; at 0x7f1ac7345cf8&gt;\n             DESCRIPTION: Repartition the RDD according to the given partitioner and, within each resulting partition,undefinedsort records by their keys.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.repartitionAndSortWithinPartitions(numPartitions=None, partitionFunc=&lt;function portable_hash at 0x7f1ac7340578&gt;, ascending=True, keyfunc=&lt;function &lt;lambda&gt; at 0x7f1ac7345cf8&gt;)",
        "text": "repartitionAndSortWithinPartitions()"
    }, {
        "name": "RDD.rightOuterJoin",
        "type": "method",
        "description": "PARAMETERS: other, numPartitions=None\n             DESCRIPTION: Perform a right outer join of self and other.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.rightOuterJoin(other, numPartitions=None)",
        "text": "rightOuterJoin()"
    }, {
        "name": "RDD.sample",
        "type": "method",
        "description": "PARAMETERS: withReplacement, fraction, seed=None\n             DESCRIPTION: Return a sampled subset of this RDD.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.sample(withReplacement, fraction, seed=None)",
        "text": "sample()"
    }, {
        "name": "RDD.sampleByKey",
        "type": "method",
        "description": "PARAMETERS: withReplacement, fractions, seed=None\n             DESCRIPTION: Return a subset of this RDD sampled by key (via stratified sampling).undefinedCreate a sample of this RDD using variable sampling rates for\ndifferent keys as specified by fractions, a key to sampling rate map.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.sampleByKey(withReplacement, fractions, seed=None)",
        "text": "sampleByKey()"
    }, {
        "name": "RDD.sampleStdev",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Compute the sample standard deviation of this RDD&#8217;s elements (whichundefinedcorrects for bias in estimating the standard deviation by dividing by\nN-1 instead of N).",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.sampleStdev()",
        "text": "sampleStdev()"
    }, {
        "name": "RDD.sampleVariance",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Compute the sample variance of this RDD&#8217;s elements (which correctsundefinedfor bias in estimating the variance by dividing by N-1 instead of N).",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.sampleVariance()",
        "text": "sampleVariance()"
    }, {
        "name": "RDD.saveAsHadoopDataset",
        "type": "method",
        "description": "PARAMETERS: conf, keyConverter=None, valueConverter=None\n             DESCRIPTION: Output a Python RDD of key-value pairs (of form RDD[(K, V)]) to any Hadoop fileundefinedsystem, using the old Hadoop OutputFormat API (mapred package). Keys/values are\nconverted for output using either user specified converters or, by default,\norg.apache.spark.api.python.JavaToWritableConverter.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.saveAsHadoopDataset(conf, keyConverter=None, valueConverter=None)",
        "text": "saveAsHadoopDataset()"
    }, {
        "name": "RDD.saveAsHadoopFile",
        "type": "method",
        "description": "PARAMETERS: path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None, compressionCodecClass=None\n             DESCRIPTION: Output a Python RDD of key-value pairs (of form RDD[(K, V)]) to any Hadoop fileundefinedsystem, using the old Hadoop OutputFormat API (mapred package). Key and value types\nwill be inferred if not specified. Keys and values are converted for output using either\nuser specified converters or org.apache.spark.api.python.JavaToWritableConverter. The\nconf is applied on top of the base Hadoop conf associated with the SparkContext\nof this RDD to create a merged Hadoop MapReduce job configuration for saving the data.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.saveAsHadoopFile(path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None, compressionCodecClass=None)",
        "text": "saveAsHadoopFile()"
    }, {
        "name": "RDD.saveAsNewAPIHadoopDataset",
        "type": "method",
        "description": "PARAMETERS: conf, keyConverter=None, valueConverter=None\n             DESCRIPTION: Output a Python RDD of key-value pairs (of form RDD[(K, V)]) to any Hadoop fileundefinedsystem, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are\nconverted for output using either user specified converters or, by default,\norg.apache.spark.api.python.JavaToWritableConverter.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.saveAsNewAPIHadoopDataset(conf, keyConverter=None, valueConverter=None)",
        "text": "saveAsNewAPIHadoopDataset()"
    }, {
        "name": "RDD.saveAsNewAPIHadoopFile",
        "type": "method",
        "description": "PARAMETERS: path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None\n             DESCRIPTION: Output a Python RDD of key-value pairs (of form RDD[(K, V)]) to any Hadoop fileundefinedsystem, using the new Hadoop OutputFormat API (mapreduce package). Key and value types\nwill be inferred if not specified. Keys and values are converted for output using either\nuser specified converters or org.apache.spark.api.python.JavaToWritableConverter. The\nconf is applied on top of the base Hadoop conf associated with the SparkContext\nof this RDD to create a merged Hadoop MapReduce job configuration for saving the data.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.saveAsNewAPIHadoopFile(path, outputFormatClass, keyClass=None, valueClass=None, keyConverter=None, valueConverter=None, conf=None)",
        "text": "saveAsNewAPIHadoopFile()"
    }, {
        "name": "RDD.saveAsPickleFile",
        "type": "method",
        "description": "PARAMETERS: path, batchSize=10\n             DESCRIPTION: Save this RDD as a SequenceFile of serialized objects. The serializerundefinedused is pyspark.serializers.PickleSerializer, default batch size\nis 10.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.saveAsPickleFile(path, batchSize=10)",
        "text": "saveAsPickleFile()"
    }, {
        "name": "RDD.saveAsSequenceFile",
        "type": "method",
        "description": "PARAMETERS: path, compressionCodecClass=None\n             DESCRIPTION: Output a Python RDD of key-value pairs (of form RDD[(K, V)]) to any Hadoop fileundefinedsystem, using the org.apache.hadoop.io.Writable types that we convert from the\nRDD&#8217;s key and value types. The mechanism is as follows:",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.saveAsSequenceFile(path, compressionCodecClass=None)",
        "text": "saveAsSequenceFile()"
    }, {
        "name": "RDD.saveAsTextFile",
        "type": "method",
        "description": "PARAMETERS: path, compressionCodecClass=None\n             DESCRIPTION: Save this RDD as a text file, using string representations of elements.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.saveAsTextFile(path, compressionCodecClass=None)",
        "text": "saveAsTextFile()"
    }, {
        "name": "RDD.setName",
        "type": "method",
        "description": "PARAMETERS: name\n             DESCRIPTION: Assign a name to this RDD.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.setName(name)",
        "text": "setName()"
    }, {
        "name": "RDD.sortBy",
        "type": "method",
        "description": "PARAMETERS: keyfunc, ascending=True, numPartitions=None\n             DESCRIPTION: Sorts this RDD by the given keyfunc",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.sortBy(keyfunc, ascending=True, numPartitions=None)",
        "text": "sortBy()"
    }, {
        "name": "RDD.sortByKey",
        "type": "method",
        "description": "PARAMETERS: ascending=True, numPartitions=None, keyfunc=&lt;function &lt;lambda&gt; at 0x7f1ac7345de8&gt;\n             DESCRIPTION: Sorts this RDD, which is assumed to consist of (key, value) pairs.undefined# noqa",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.sortByKey(ascending=True, numPartitions=None, keyfunc=&lt;function &lt;lambda&gt; at 0x7f1ac7345de8&gt;)",
        "text": "sortByKey()"
    }, {
        "name": "RDD.stats",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Return a StatCounter object that captures the mean, varianceundefinedand count of the RDD&#8217;s elements in one operation.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.stats()",
        "text": "stats()"
    }, {
        "name": "RDD.stdev",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Compute the standard deviation of this RDD&#8217;s elements.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.stdev()",
        "text": "stdev()"
    }, {
        "name": "RDD.subtract",
        "type": "method",
        "description": "PARAMETERS: other, numPartitions=None\n             DESCRIPTION: Return each value in self that is not contained in other.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.subtract(other, numPartitions=None)",
        "text": "subtract()"
    }, {
        "name": "RDD.subtractByKey",
        "type": "method",
        "description": "PARAMETERS: other, numPartitions=None\n             DESCRIPTION: Return each (key, value) pair in self that has no pair with matchingundefinedkey in other.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.subtractByKey(other, numPartitions=None)",
        "text": "subtractByKey()"
    }, {
        "name": "RDD.sum",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Add up the elements in this RDD.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.sum()",
        "text": "sum()"
    }, {
        "name": "RDD.sumApprox",
        "type": "method",
        "description": "PARAMETERS: timeout, confidence=0.95\n             DESCRIPTION: Approximate operation to return the sum within a timeoutundefinedor meet the confidence.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.sumApprox(timeout, confidence=0.95)",
        "text": "sumApprox()"
    }, {
        "name": "RDD.take",
        "type": "method",
        "description": "PARAMETERS: num\n             DESCRIPTION: Take the first num elements of the RDD.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.take(num)",
        "text": "take()"
    }, {
        "name": "RDD.takeOrdered",
        "type": "method",
        "description": "PARAMETERS: num, key=None\n             DESCRIPTION: Get the N elements from a RDD ordered in ascending order or asundefinedspecified by the optional key function.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.takeOrdered(num, key=None)",
        "text": "takeOrdered()"
    }, {
        "name": "RDD.takeSample",
        "type": "method",
        "description": "PARAMETERS: withReplacement, num, seed=None\n             DESCRIPTION: Return a fixed-size sampled subset of this RDD.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.takeSample(withReplacement, num, seed=None)",
        "text": "takeSample()"
    }, {
        "name": "RDD.toDebugString",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: A description of this RDD and its recursive dependencies for debugging.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.toDebugString()",
        "text": "toDebugString()"
    }, {
        "name": "RDD.toLocalIterator",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Return an iterator that contains all of the elements in this RDD.undefinedThe iterator will consume as much memory as the largest partition in this RDD.\n&gt;&gt;&gt; rdd = sc.parallelize(range(10))\n&gt;&gt;&gt; [x for x in rdd.toLocalIterator()]\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.toLocalIterator()",
        "text": "toLocalIterator()"
    }, {
        "name": "RDD.top",
        "type": "method",
        "description": "PARAMETERS: num, key=None\n             DESCRIPTION: Get the top N elements from a RDD.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.top(num, key=None)",
        "text": "top()"
    }, {
        "name": "RDD.treeAggregate",
        "type": "method",
        "description": "PARAMETERS: zeroValue, seqOp, combOp, depth=2\n             DESCRIPTION: Aggregates the elements of this RDD in a multi-level treeundefinedpattern.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.treeAggregate(zeroValue, seqOp, combOp, depth=2)",
        "text": "treeAggregate()"
    }, {
        "name": "RDD.treeReduce",
        "type": "method",
        "description": "PARAMETERS: f, depth=2\n             DESCRIPTION: Reduces the elements of this RDD in a multi-level tree pattern.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.treeReduce(f, depth=2)",
        "text": "treeReduce()"
    }, {
        "name": "RDD.union",
        "type": "method",
        "description": "PARAMETERS: other\n             DESCRIPTION: Return the union of this RDD and another one.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.union(other)",
        "text": "union()"
    }, {
        "name": "RDD.unpersist",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Mark the RDD as non-persistent, and remove all blocks for it fromundefinedmemory and disk.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.unpersist()",
        "text": "unpersist()"
    }, {
        "name": "RDD.values",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Return an RDD with the values of each tuple.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.values()",
        "text": "values()"
    }, {
        "name": "RDD.variance",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Compute the variance of this RDD&#8217;s elements.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.variance()",
        "text": "variance()"
    }, {
        "name": "RDD.zip",
        "type": "method",
        "description": "PARAMETERS: other\n             DESCRIPTION: Zips this RDD with another one, returning key-value pairs with theundefinedfirst element in each RDD second element in each RDD, etc. Assumes\nthat the two RDDs have the same number of partitions and the same\nnumber of elements in each partition (e.g. one was made through\na map on the other).",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.zip(other)",
        "text": "zip()"
    }, {
        "name": "RDD.zipWithIndex",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Zips this RDD with its element indices.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.zipWithIndex()",
        "text": "zipWithIndex()"
    }, {
        "name": "RDD.zipWithUniqueId",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Zips this RDD with generated unique Long ids.",
        "leftLabel": "RDD",
        "rightLabel": "method | PySpark",
        "displayText": "RDD.zipWithUniqueId()",
        "text": "zipWithUniqueId()"
    }, {
        "name": "StorageLevel",
        "type": "class",
        "description": "PARAMETERS: useDisk, useMemory, useOffHeap, deserialized, replication=1\n             DESCRIPTION: Flags for controlling the storage of an RDD. Each StorageLevel records whether to use memory,undefinedwhether to drop the RDD to disk if it falls out of memory, whether to keep the data in memory\nin a serialized format, and whether to replicate the RDD partitions on multiple nodes.\nAlso contains static constants for some commonly used storage levels, such as MEMORY_ONLY.",
        "rightLabel": "class | PySpark",
        "displayText": "StorageLevel(useDisk, useMemory, useOffHeap, deserialized, replication=1)",
        "text": "StorageLevel()",
        "leftLabel": "StorageLevel"
    }, {
        "name": "StorageLevel.DISK_ONLY",
        "type": "value",
        "leftLabel": "StorageLevel",
        "rightLabel": "attribute | PySpark",
        "displayText": "StorageLevel.DISK_ONLY",
        "text": "DISK_ONLY"
    }, {
        "name": "StorageLevel.DISK_ONLY_2",
        "type": "value",
        "leftLabel": "StorageLevel",
        "rightLabel": "attribute | PySpark",
        "displayText": "StorageLevel.DISK_ONLY_2",
        "text": "DISK_ONLY_2"
    }, {
        "name": "StorageLevel.MEMORY_AND_DISK",
        "type": "value",
        "leftLabel": "StorageLevel",
        "rightLabel": "attribute | PySpark",
        "displayText": "StorageLevel.MEMORY_AND_DISK",
        "text": "MEMORY_AND_DISK"
    }, {
        "name": "StorageLevel.MEMORY_AND_DISK_2",
        "type": "value",
        "leftLabel": "StorageLevel",
        "rightLabel": "attribute | PySpark",
        "displayText": "StorageLevel.MEMORY_AND_DISK_2",
        "text": "MEMORY_AND_DISK_2"
    }, {
        "name": "StorageLevel.MEMORY_AND_DISK_SER",
        "type": "value",
        "leftLabel": "StorageLevel",
        "rightLabel": "attribute | PySpark",
        "displayText": "StorageLevel.MEMORY_AND_DISK_SER",
        "text": "MEMORY_AND_DISK_SER"
    }, {
        "name": "StorageLevel.MEMORY_AND_DISK_SER_2",
        "type": "value",
        "leftLabel": "StorageLevel",
        "rightLabel": "attribute | PySpark",
        "displayText": "StorageLevel.MEMORY_AND_DISK_SER_2",
        "text": "MEMORY_AND_DISK_SER_2"
    }, {
        "name": "StorageLevel.MEMORY_ONLY",
        "type": "value",
        "leftLabel": "StorageLevel",
        "rightLabel": "attribute | PySpark",
        "displayText": "StorageLevel.MEMORY_ONLY",
        "text": "MEMORY_ONLY"
    }, {
        "name": "StorageLevel.MEMORY_ONLY_2",
        "type": "value",
        "leftLabel": "StorageLevel",
        "rightLabel": "attribute | PySpark",
        "displayText": "StorageLevel.MEMORY_ONLY_2",
        "text": "MEMORY_ONLY_2"
    }, {
        "name": "StorageLevel.MEMORY_ONLY_SER",
        "type": "value",
        "leftLabel": "StorageLevel",
        "rightLabel": "attribute | PySpark",
        "displayText": "StorageLevel.MEMORY_ONLY_SER",
        "text": "MEMORY_ONLY_SER"
    }, {
        "name": "StorageLevel.MEMORY_ONLY_SER_2",
        "type": "value",
        "leftLabel": "StorageLevel",
        "rightLabel": "attribute | PySpark",
        "displayText": "StorageLevel.MEMORY_ONLY_SER_2",
        "text": "MEMORY_ONLY_SER_2"
    }, {
        "name": "StorageLevel.OFF_HEAP",
        "type": "value",
        "leftLabel": "StorageLevel",
        "rightLabel": "attribute | PySpark",
        "displayText": "StorageLevel.OFF_HEAP",
        "text": "OFF_HEAP"
    }, {
        "name": "Broadcast",
        "type": "class",
        "description": "PARAMETERS: sc=None, value=None, pickle_registry=None, path=None\n             DESCRIPTION: A broadcast variable created with SparkContext.broadcast().undefinedAccess its value through value.",
        "rightLabel": "class | PySpark",
        "displayText": "Broadcast(sc=None, value=None, pickle_registry=None, path=None)",
        "text": "Broadcast()",
        "leftLabel": "Broadcast"
    }, {
        "name": "Broadcast.value",
        "type": "value",
        "description": "PARAMETERS: \n             DESCRIPTION: Return the broadcasted value",
        "leftLabel": "Broadcast",
        "rightLabel": "attribute | PySpark",
        "displayText": "Broadcast.value",
        "text": "value"
    }, {
        "name": "Broadcast.dump",
        "type": "method",
        "leftLabel": "Broadcast",
        "rightLabel": "method | PySpark",
        "displayText": "Broadcast.dump(value, f)",
        "text": "dump()"
    }, {
        "name": "Broadcast.load",
        "type": "method",
        "leftLabel": "Broadcast",
        "rightLabel": "method | PySpark",
        "displayText": "Broadcast.load(path)",
        "text": "load()"
    }, {
        "name": "Broadcast.unpersist",
        "type": "method",
        "description": "PARAMETERS: blocking=False\n             DESCRIPTION: Delete cached copies of this broadcast on the executors.",
        "leftLabel": "Broadcast",
        "rightLabel": "method | PySpark",
        "displayText": "Broadcast.unpersist(blocking=False)",
        "text": "unpersist()"
    }, {
        "name": "Accumulator",
        "type": "class",
        "description": "PARAMETERS: aid, value, accum_param\n             DESCRIPTION: A shared variable that can be accumulated, i.e., has a commutative and associative &#8220;add&#8221;undefinedoperation. Worker tasks on a Spark cluster can add values to an Accumulator with the +=\noperator, but only the driver program is allowed to access its value, using value.\nUpdates from the workers get propagated automatically to the driver program.",
        "rightLabel": "class | PySpark",
        "displayText": "Accumulator(aid, value, accum_param)",
        "text": "Accumulator()",
        "leftLabel": "Accumulator"
    }, {
        "name": "Accumulator.value",
        "type": "value",
        "description": "PARAMETERS: \n             DESCRIPTION: Get the accumulator&#8217;s value; only usable in driver program",
        "leftLabel": "Accumulator",
        "rightLabel": "attribute | PySpark",
        "displayText": "Accumulator.value",
        "text": "value"
    }, {
        "name": "Accumulator.add",
        "type": "method",
        "description": "PARAMETERS: term\n             DESCRIPTION: Adds a term to this accumulator&#8217;s value",
        "leftLabel": "Accumulator",
        "rightLabel": "method | PySpark",
        "displayText": "Accumulator.add(term)",
        "text": "add()"
    }, {
        "name": "AccumulatorParam",
        "type": "class",
        "description": "PARAMETERS: \n             DESCRIPTION: Helper object that defines how to accumulate values of a given type.",
        "rightLabel": "class | PySpark",
        "displayText": "AccumulatorParam()",
        "text": "AccumulatorParam()",
        "leftLabel": "AccumulatorParam"
    }, {
        "name": "AccumulatorParam.addInPlace",
        "type": "method",
        "description": "PARAMETERS: value1, value2\n             DESCRIPTION: Add two values of the accumulator&#8217;s data type, returning a new value;undefinedfor efficiency, can also update value1 in place and return it.",
        "leftLabel": "AccumulatorParam",
        "rightLabel": "method | PySpark",
        "displayText": "AccumulatorParam.addInPlace(value1, value2)",
        "text": "addInPlace()"
    }, {
        "name": "AccumulatorParam.zero",
        "type": "method",
        "description": "PARAMETERS: value\n             DESCRIPTION: Provide a &#8220;zero value&#8221; for the type, compatible in dimensions with theundefinedprovided value (e.g., a zero vector)",
        "leftLabel": "AccumulatorParam",
        "rightLabel": "method | PySpark",
        "displayText": "AccumulatorParam.zero(value)",
        "text": "zero()"
    }, {
        "name": "MarshalSerializer",
        "type": "class",
        "description": "PARAMETERS: \n             DESCRIPTION: Serializes objects using Python&#8217;s Marshal serializer:",
        "rightLabel": "class | PySpark",
        "displayText": "MarshalSerializer()",
        "text": "MarshalSerializer()",
        "leftLabel": "MarshalSerializer"
    }, {
        "name": "MarshalSerializer.dumps",
        "type": "method",
        "leftLabel": "MarshalSerializer",
        "rightLabel": "method | PySpark",
        "displayText": "MarshalSerializer.dumps(obj)",
        "text": "dumps()"
    }, {
        "name": "MarshalSerializer.loads",
        "type": "method",
        "leftLabel": "MarshalSerializer",
        "rightLabel": "method | PySpark",
        "displayText": "MarshalSerializer.loads(obj)",
        "text": "loads()"
    }, {
        "name": "PickleSerializer",
        "type": "class",
        "description": "PARAMETERS: \n             DESCRIPTION: Serializes objects using Python&#8217;s pickle serializer:",
        "rightLabel": "class | PySpark",
        "displayText": "PickleSerializer()",
        "text": "PickleSerializer()",
        "leftLabel": "PickleSerializer"
    }, {
        "name": "PickleSerializer.dumps",
        "type": "method",
        "leftLabel": "PickleSerializer",
        "rightLabel": "method | PySpark",
        "displayText": "PickleSerializer.dumps(obj)",
        "text": "dumps()"
    }, {
        "name": "PickleSerializer.loads",
        "type": "method",
        "leftLabel": "PickleSerializer",
        "rightLabel": "method | PySpark",
        "displayText": "PickleSerializer.loads(obj, encoding=None)",
        "text": "loads()"
    }, {
        "name": "StatusTracker",
        "type": "class",
        "description": "PARAMETERS: jtracker\n             DESCRIPTION: Low-level status reporting APIs for monitoring job and stage progress.",
        "rightLabel": "class | PySpark",
        "displayText": "StatusTracker(jtracker)",
        "text": "StatusTracker()",
        "leftLabel": "StatusTracker"
    }, {
        "name": "StatusTracker.getActiveJobsIds",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Returns an array containing the ids of all active jobs.",
        "leftLabel": "StatusTracker",
        "rightLabel": "method | PySpark",
        "displayText": "StatusTracker.getActiveJobsIds()",
        "text": "getActiveJobsIds()"
    }, {
        "name": "StatusTracker.getActiveStageIds",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Returns an array containing the ids of all active stages.",
        "leftLabel": "StatusTracker",
        "rightLabel": "method | PySpark",
        "displayText": "StatusTracker.getActiveStageIds()",
        "text": "getActiveStageIds()"
    }, {
        "name": "StatusTracker.getJobIdsForGroup",
        "type": "method",
        "description": "PARAMETERS: jobGroup=None\n             DESCRIPTION: Return a list of all known jobs in a particular job group.  IfundefinedjobGroup is None, then returns all known jobs that are not\nassociated with a job group.",
        "leftLabel": "StatusTracker",
        "rightLabel": "method | PySpark",
        "displayText": "StatusTracker.getJobIdsForGroup(jobGroup=None)",
        "text": "getJobIdsForGroup()"
    }, {
        "name": "StatusTracker.getJobInfo",
        "type": "method",
        "description": "PARAMETERS: jobId\n             DESCRIPTION: Returns a SparkJobInfo object, or None if the job infoundefinedcould not be found or was garbage collected.",
        "leftLabel": "StatusTracker",
        "rightLabel": "method | PySpark",
        "displayText": "StatusTracker.getJobInfo(jobId)",
        "text": "getJobInfo()"
    }, {
        "name": "StatusTracker.getStageInfo",
        "type": "method",
        "description": "PARAMETERS: stageId\n             DESCRIPTION: Returns a SparkStageInfo object, or None if the stageundefinedinfo could not be found or was garbage collected.",
        "leftLabel": "StatusTracker",
        "rightLabel": "method | PySpark",
        "displayText": "StatusTracker.getStageInfo(stageId)",
        "text": "getStageInfo()"
    }, {
        "name": "SparkJobInfo",
        "type": "class",
        "description": "PARAMETERS: \n             DESCRIPTION: Exposes information about Spark Jobs.",
        "rightLabel": "class | PySpark",
        "displayText": "SparkJobInfo()",
        "text": "SparkJobInfo()",
        "leftLabel": "SparkJobInfo"
    }, {
        "name": "SparkStageInfo",
        "type": "class",
        "description": "PARAMETERS: \n             DESCRIPTION: Exposes information about Spark Stages.",
        "rightLabel": "class | PySpark",
        "displayText": "SparkStageInfo()",
        "text": "SparkStageInfo()",
        "leftLabel": "SparkStageInfo"
    }, {
        "name": "Profiler",
        "type": "class",
        "description": "PARAMETERS: ctx\n             DESCRIPTION: PySpark supports custom profilers, this is to allow for different profilers toundefinedbe used as well as outputting to different formats than what is provided in the\nBasicProfiler.",
        "rightLabel": "class | PySpark",
        "displayText": "Profiler(ctx)",
        "text": "Profiler()",
        "leftLabel": "Profiler"
    }, {
        "name": "Profiler.dump",
        "type": "method",
        "description": "PARAMETERS: id, path\n             DESCRIPTION: Dump the profile into path, id is the RDD id",
        "leftLabel": "Profiler",
        "rightLabel": "method | PySpark",
        "displayText": "Profiler.dump(id, path)",
        "text": "dump()"
    }, {
        "name": "Profiler.profile",
        "type": "method",
        "description": "PARAMETERS: func\n             DESCRIPTION: Do profiling on the function func",
        "leftLabel": "Profiler",
        "rightLabel": "method | PySpark",
        "displayText": "Profiler.profile(func)",
        "text": "profile()"
    }, {
        "name": "Profiler.show",
        "type": "method",
        "description": "PARAMETERS: id\n             DESCRIPTION: Print the profile stats to stdout, id is the RDD id",
        "leftLabel": "Profiler",
        "rightLabel": "method | PySpark",
        "displayText": "Profiler.show(id)",
        "text": "show()"
    }, {
        "name": "Profiler.stats",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Return the collected profiling stats (pstats.Stats)",
        "leftLabel": "Profiler",
        "rightLabel": "method | PySpark",
        "displayText": "Profiler.stats()",
        "text": "stats()"
    }, {
        "name": "BasicProfiler",
        "type": "class",
        "description": "PARAMETERS: ctx\n             DESCRIPTION: BasicProfiler is the default profiler, which is implemented based onundefinedcProfile and Accumulator",
        "rightLabel": "class | PySpark",
        "displayText": "BasicProfiler(ctx)",
        "text": "BasicProfiler()",
        "leftLabel": "BasicProfiler"
    }, {
        "name": "BasicProfiler.profile",
        "type": "method",
        "description": "PARAMETERS: func\n             DESCRIPTION: Runs and profiles the method to_profile passed in. A profile object is returned.",
        "leftLabel": "BasicProfiler",
        "rightLabel": "method | PySpark",
        "displayText": "BasicProfiler.profile(func)",
        "text": "profile()"
    }, {
        "name": "BasicProfiler.stats",
        "type": "method",
        "leftLabel": "BasicProfiler",
        "rightLabel": "method | PySpark",
        "displayText": "BasicProfiler.stats()",
        "text": "stats()"
    }, {
        "name": "sql.SQLContext",
        "type": "class",
        "description": "PARAMETERS: sparkContext, sqlContext=None\n             DESCRIPTION: Main entry point for Spark SQL functionality.",
        "rightLabel": "class | PySpark",
        "displayText": "sql.SQLContext(sparkContext, sqlContext=None)",
        "text": "SQLContext()",
        "leftLabel": "sql.SQLContext"
    }, {
        "name": "sql.SQLContext.read",
        "type": "value",
        "description": "PARAMETERS: df, tableName\n             DESCRIPTION: Returns a DataFrameReader that can be used to read dataundefinedin as a DataFrame.",
        "leftLabel": "sql.SQLContext",
        "rightLabel": "attribute | PySpark",
        "displayText": "sql.SQLContext.read",
        "text": "read"
    }, {
        "name": "sql.SQLContext.udf",
        "type": "value",
        "description": "PARAMETERS: tableName\n             DESCRIPTION: Returns a UDFRegistration for UDF registration.",
        "leftLabel": "sql.SQLContext",
        "rightLabel": "attribute | PySpark",
        "displayText": "sql.SQLContext.udf",
        "text": "udf"
    }, {
        "name": "sql.SQLContext.applySchema",
        "type": "method",
        "leftLabel": "sql.SQLContext",
        "rightLabel": "method | PySpark",
        "displayText": "sql.SQLContext.applySchema(rdd, schema)",
        "text": "applySchema()"
    }, {
        "name": "sql.SQLContext.cacheTable",
        "type": "method",
        "description": "PARAMETERS: tableName\n             DESCRIPTION: Caches the specified table in-memory.",
        "leftLabel": "sql.SQLContext",
        "rightLabel": "method | PySpark",
        "displayText": "sql.SQLContext.cacheTable(tableName)",
        "text": "cacheTable()"
    }, {
        "name": "sql.SQLContext.clearCache",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Removes all cached tables from the in-memory cache.",
        "leftLabel": "sql.SQLContext",
        "rightLabel": "method | PySpark",
        "displayText": "sql.SQLContext.clearCache()",
        "text": "clearCache()"
    }, {
        "name": "sql.SQLContext.createDataFrame",
        "type": "method",
        "description": "PARAMETERS: data, schema=None, samplingRatio=None\n             DESCRIPTION: Creates a DataFrame from an RDD of tuple/list,undefinedlist or pandas.DataFrame.",
        "leftLabel": "sql.SQLContext",
        "rightLabel": "method | PySpark",
        "displayText": "sql.SQLContext.createDataFrame(data, schema=None, samplingRatio=None)",
        "text": "createDataFrame()"
    }, {
        "name": "sql.SQLContext.createExternalTable",
        "type": "method",
        "description": "PARAMETERS: tableName, path=None, source=None, schema=None, **options\n             DESCRIPTION: Creates an external table based on the dataset in a data source.",
        "leftLabel": "sql.SQLContext",
        "rightLabel": "method | PySpark",
        "displayText": "sql.SQLContext.createExternalTable(tableName, path=None, source=None, schema=None, **options)",
        "text": "createExternalTable()"
    }, {
        "name": "sql.SQLContext.dropTempTable",
        "type": "method",
        "description": "PARAMETERS: tableName\n             DESCRIPTION: Remove the temp table from catalog.",
        "leftLabel": "sql.SQLContext",
        "rightLabel": "method | PySpark",
        "displayText": "sql.SQLContext.dropTempTable(tableName)",
        "text": "dropTempTable()"
    }, {
        "name": "sql.SQLContext.getConf",
        "type": "method",
        "description": "PARAMETERS: key, defaultValue\n             DESCRIPTION: Returns the value of Spark SQL configuration property for the given key.",
        "leftLabel": "sql.SQLContext",
        "rightLabel": "method | PySpark",
        "displayText": "sql.SQLContext.getConf(key, defaultValue)",
        "text": "getConf()"
    }, {
        "name": "sql.SQLContext.inferSchema",
        "type": "method",
        "leftLabel": "sql.SQLContext",
        "rightLabel": "method | PySpark",
        "displayText": "sql.SQLContext.inferSchema(rdd, samplingRatio=None)",
        "text": "inferSchema()"
    }, {
        "name": "sql.SQLContext.jsonFile",
        "type": "method",
        "description": "PARAMETERS: path, schema=None, samplingRatio=1.0\n             DESCRIPTION: Loads a text file storing one JSON object per line as a DataFrame.",
        "leftLabel": "sql.SQLContext",
        "rightLabel": "method | PySpark",
        "displayText": "sql.SQLContext.jsonFile(path, schema=None, samplingRatio=1.0)",
        "text": "jsonFile()"
    }, {
        "name": "sql.SQLContext.jsonRDD",
        "type": "method",
        "description": "PARAMETERS: rdd, schema=None, samplingRatio=1.0\n             DESCRIPTION: Loads an RDD storing one JSON object per string as a DataFrame.",
        "leftLabel": "sql.SQLContext",
        "rightLabel": "method | PySpark",
        "displayText": "sql.SQLContext.jsonRDD(rdd, schema=None, samplingRatio=1.0)",
        "text": "jsonRDD()"
    }, {
        "name": "sql.SQLContext.load",
        "type": "method",
        "description": "PARAMETERS: path=None, source=None, schema=None, **options\n             DESCRIPTION: Returns the dataset in a data source as a DataFrame.",
        "leftLabel": "sql.SQLContext",
        "rightLabel": "method | PySpark",
        "displayText": "sql.SQLContext.load(path=None, source=None, schema=None, **options)",
        "text": "load()"
    }, {
        "name": "sql.SQLContext.newSession",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Returns a new SQLContext as new session, that has separate SQLConf,undefinedregistered temporary tables and UDFs, but shared SparkContext and\ntable cache.",
        "leftLabel": "sql.SQLContext",
        "rightLabel": "method | PySpark",
        "displayText": "sql.SQLContext.newSession()",
        "text": "newSession()"
    }, {
        "name": "sql.SQLContext.parquetFile",
        "type": "method",
        "description": "PARAMETERS: *paths\n             DESCRIPTION: Loads a Parquet file, returning the result as a DataFrame.",
        "leftLabel": "sql.SQLContext",
        "rightLabel": "method | PySpark",
        "displayText": "sql.SQLContext.parquetFile(*paths)",
        "text": "parquetFile()"
    }, {
        "name": "sql.SQLContext.range",
        "type": "method",
        "description": "PARAMETERS: start, end=None, step=1, numPartitions=None\n             DESCRIPTION: Create a DataFrame with single LongType column named id,undefinedcontaining elements in a range from start to end (exclusive) with\nstep value step.",
        "leftLabel": "sql.SQLContext",
        "rightLabel": "method | PySpark",
        "displayText": "sql.SQLContext.range(start, end=None, step=1, numPartitions=None)",
        "text": "range()"
    }, {
        "name": "sql.SQLContext.registerDataFrameAsTable",
        "type": "method",
        "description": "PARAMETERS: df, tableName\n             DESCRIPTION: Registers the given DataFrame as a temporary table in the catalog.",
        "leftLabel": "sql.SQLContext",
        "rightLabel": "method | PySpark",
        "displayText": "sql.SQLContext.registerDataFrameAsTable(df, tableName)",
        "text": "registerDataFrameAsTable()"
    }, {
        "name": "sql.SQLContext.registerFunction",
        "type": "method",
        "description": "PARAMETERS: name, f, returnType=StringType\n             DESCRIPTION: Registers a python function (including lambda function) as a UDFundefinedso it can be used in SQL statements.",
        "leftLabel": "sql.SQLContext",
        "rightLabel": "method | PySpark",
        "displayText": "sql.SQLContext.registerFunction(name, f, returnType=StringType)",
        "text": "registerFunction()"
    }, {
        "name": "sql.SQLContext.setConf",
        "type": "method",
        "description": "PARAMETERS: key, value\n             DESCRIPTION: Sets the given Spark SQL configuration property.",
        "leftLabel": "sql.SQLContext",
        "rightLabel": "method | PySpark",
        "displayText": "sql.SQLContext.setConf(key, value)",
        "text": "setConf()"
    }, {
        "name": "sql.SQLContext.sql",
        "type": "method",
        "description": "PARAMETERS: sqlQuery\n             DESCRIPTION: Returns a DataFrame representing the result of the given query.",
        "leftLabel": "sql.SQLContext",
        "rightLabel": "method | PySpark",
        "displayText": "sql.SQLContext.sql(sqlQuery)",
        "text": "sql()"
    }, {
        "name": "sql.SQLContext.table",
        "type": "method",
        "description": "PARAMETERS: tableName\n             DESCRIPTION: Returns the specified table as a DataFrame.",
        "leftLabel": "sql.SQLContext",
        "rightLabel": "method | PySpark",
        "displayText": "sql.SQLContext.table(tableName)",
        "text": "table()"
    }, {
        "name": "sql.SQLContext.tableNames",
        "type": "method",
        "description": "PARAMETERS: dbName=None\n             DESCRIPTION: Returns a list of names of tables in the database dbName.",
        "leftLabel": "sql.SQLContext",
        "rightLabel": "method | PySpark",
        "displayText": "sql.SQLContext.tableNames(dbName=None)",
        "text": "tableNames()"
    }, {
        "name": "sql.SQLContext.tables",
        "type": "method",
        "description": "PARAMETERS: dbName=None\n             DESCRIPTION: Returns a DataFrame containing names of tables in the given database.",
        "leftLabel": "sql.SQLContext",
        "rightLabel": "method | PySpark",
        "displayText": "sql.SQLContext.tables(dbName=None)",
        "text": "tables()"
    }, {
        "name": "sql.SQLContext.uncacheTable",
        "type": "method",
        "description": "PARAMETERS: tableName\n             DESCRIPTION: Removes the specified table from the in-memory cache.",
        "leftLabel": "sql.SQLContext",
        "rightLabel": "method | PySpark",
        "displayText": "sql.SQLContext.uncacheTable(tableName)",
        "text": "uncacheTable()"
    }, {
        "name": "sql.HiveContext",
        "type": "class",
        "description": "PARAMETERS: sparkContext, hiveContext=None\n             DESCRIPTION: A variant of Spark SQL that integrates with data stored in Hive.",
        "rightLabel": "class | PySpark",
        "displayText": "sql.HiveContext(sparkContext, hiveContext=None)",
        "text": "HiveContext()",
        "leftLabel": "sql.HiveContext"
    }, {
        "name": "sql.HiveContext.refreshTable",
        "type": "method",
        "description": "PARAMETERS: tableName\n             DESCRIPTION: Invalidate and refresh all the cached the metadata of the givenundefinedtable. For performance reasons, Spark SQL or the external data source\nlibrary it uses might cache certain metadata about a table, such as the\nlocation of blocks. When those change outside of Spark SQL, users should\ncall this function to invalidate the cache.",
        "leftLabel": "sql.HiveContext",
        "rightLabel": "method | PySpark",
        "displayText": "sql.HiveContext.refreshTable(tableName)",
        "text": "refreshTable()"
    }, {
        "name": "sql.DataFrame",
        "type": "class",
        "description": "PARAMETERS: jdf, sql_ctx\n             DESCRIPTION: A distributed collection of data grouped into named columns.",
        "rightLabel": "class | PySpark",
        "displayText": "sql.DataFrame(jdf, sql_ctx)",
        "text": "DataFrame()",
        "leftLabel": "sql.DataFrame"
    }, {
        "name": "sql.DataFrame.columns",
        "type": "value",
        "description": "PARAMETERS: col1, col2, method=None\n             DESCRIPTION: Returns all column names as a list.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "attribute | PySpark",
        "displayText": "sql.DataFrame.columns",
        "text": "columns"
    }, {
        "name": "sql.DataFrame.dtypes",
        "type": "value",
        "description": "PARAMETERS: extended=False\n             DESCRIPTION: Returns all column names and their data types as a list.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "attribute | PySpark",
        "displayText": "sql.DataFrame.dtypes",
        "text": "dtypes"
    }, {
        "name": "sql.DataFrame.na",
        "type": "value",
        "description": "PARAMETERS: *cols, **kwargs\n             DESCRIPTION: Returns a DataFrameNaFunctions for handling missing values.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "attribute | PySpark",
        "displayText": "sql.DataFrame.na",
        "text": "na"
    }, {
        "name": "sql.DataFrame.rdd",
        "type": "value",
        "description": "PARAMETERS: name\n             DESCRIPTION: Returns the content as an pyspark.RDD of Row.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "attribute | PySpark",
        "displayText": "sql.DataFrame.rdd",
        "text": "rdd"
    }, {
        "name": "sql.DataFrame.schema",
        "type": "value",
        "description": "PARAMETERS: *cols\n             DESCRIPTION: Returns the schema of this DataFrame as a types.StructType.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "attribute | PySpark",
        "displayText": "sql.DataFrame.schema",
        "text": "schema"
    }, {
        "name": "sql.DataFrame.stat",
        "type": "value",
        "description": "PARAMETERS: other\n             DESCRIPTION: Returns a DataFrameStatFunctions for statistic functions.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "attribute | PySpark",
        "displayText": "sql.DataFrame.stat",
        "text": "stat"
    }, {
        "name": "sql.DataFrame.write",
        "type": "value",
        "description": "PARAMETERS: \n             DESCRIPTION: Interface for saving the content of the DataFrame out into external storage.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "attribute | PySpark",
        "displayText": "sql.DataFrame.write",
        "text": "write"
    }, {
        "name": "sql.DataFrame.agg",
        "type": "method",
        "description": "PARAMETERS: *exprs\n             DESCRIPTION: Aggregate on the entire DataFrame without groupsundefined(shorthand for df.groupBy.agg()).",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.agg(*exprs)",
        "text": "agg()"
    }, {
        "name": "sql.DataFrame.alias",
        "type": "method",
        "description": "PARAMETERS: alias\n             DESCRIPTION: Returns a new DataFrame with an alias set.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.alias(alias)",
        "text": "alias()"
    }, {
        "name": "sql.DataFrame.cache",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Persists with the default storage level (MEMORY_ONLY_SER).",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.cache()",
        "text": "cache()"
    }, {
        "name": "sql.DataFrame.coalesce",
        "type": "method",
        "description": "PARAMETERS: numPartitions\n             DESCRIPTION: Returns a new DataFrame that has exactly numPartitions partitions.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.coalesce(numPartitions)",
        "text": "coalesce()"
    }, {
        "name": "sql.DataFrame.collect",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Returns all the records as a list of Row.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.collect()",
        "text": "collect()"
    }, {
        "name": "sql.DataFrame.corr",
        "type": "method",
        "description": "PARAMETERS: col1, col2, method=None\n             DESCRIPTION: Calculates the correlation of two columns of a DataFrame as a double value.undefinedCurrently only supports the Pearson Correlation Coefficient.\nDataFrame.corr() and DataFrameStatFunctions.corr() are aliases of each other.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.corr(col1, col2, method=None)",
        "text": "corr()"
    }, {
        "name": "sql.DataFrame.count",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Returns the number of rows in this DataFrame.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.count()",
        "text": "count()"
    }, {
        "name": "sql.DataFrame.cov",
        "type": "method",
        "description": "PARAMETERS: col1, col2\n             DESCRIPTION: Calculate the sample covariance for the given columns, specified by their names, as aundefineddouble value. DataFrame.cov() and DataFrameStatFunctions.cov() are aliases.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.cov(col1, col2)",
        "text": "cov()"
    }, {
        "name": "sql.DataFrame.crosstab",
        "type": "method",
        "description": "PARAMETERS: col1, col2\n             DESCRIPTION: Computes a pair-wise frequency table of the given columns. Also known as a contingencyundefinedtable. The number of distinct values for each column should be less than 1e4. At most 1e6\nnon-zero pair frequencies will be returned.\nThe first column of each row will be the distinct values of col1 and the column names\nwill be the distinct values of col2. The name of the first column will be $col1_$col2.\nPairs that have no occurrences will have zero as their counts.\nDataFrame.crosstab() and DataFrameStatFunctions.crosstab() are aliases.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.crosstab(col1, col2)",
        "text": "crosstab()"
    }, {
        "name": "sql.DataFrame.cube",
        "type": "method",
        "description": "PARAMETERS: *cols\n             DESCRIPTION: Create a multi-dimensional cube for the current DataFrame usingundefinedthe specified columns, so we can run aggregation on them.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.cube(*cols)",
        "text": "cube()"
    }, {
        "name": "sql.DataFrame.describe",
        "type": "method",
        "description": "PARAMETERS: *cols\n             DESCRIPTION: Computes statistics for numeric columns.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.describe(*cols)",
        "text": "describe()"
    }, {
        "name": "sql.DataFrame.distinct",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Returns a new DataFrame containing the distinct rows in this DataFrame.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.distinct()",
        "text": "distinct()"
    }, {
        "name": "sql.DataFrame.drop",
        "type": "method",
        "description": "PARAMETERS: col\n             DESCRIPTION: Returns a new DataFrame that drops the specified column.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.drop(col)",
        "text": "drop()"
    }, {
        "name": "sql.DataFrame.dropDuplicates",
        "type": "method",
        "description": "PARAMETERS: subset=None\n             DESCRIPTION: Return a new DataFrame with duplicate rows removed,undefinedoptionally only considering certain columns.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.dropDuplicates(subset=None)",
        "text": "dropDuplicates()"
    }, {
        "name": "sql.DataFrame.drop_duplicates",
        "type": "method",
        "description": "PARAMETERS: subset=None\n             DESCRIPTION: Return a new DataFrame with duplicate rows removed,undefinedoptionally only considering certain columns.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.drop_duplicates(subset=None)",
        "text": "drop_duplicates()"
    }, {
        "name": "sql.DataFrame.dropna",
        "type": "method",
        "description": "PARAMETERS: how='any', thresh=None, subset=None\n             DESCRIPTION: Returns a new DataFrame omitting rows with null values.undefinedDataFrame.dropna() and DataFrameNaFunctions.drop() are aliases of each other.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.dropna(how='any', thresh=None, subset=None)",
        "text": "dropna()"
    }, {
        "name": "sql.DataFrame.explain",
        "type": "method",
        "description": "PARAMETERS: extended=False\n             DESCRIPTION: Prints the (logical and physical) plans to the console for debugging purpose.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.explain(extended=False)",
        "text": "explain()"
    }, {
        "name": "sql.DataFrame.fillna",
        "type": "method",
        "description": "PARAMETERS: value, subset=None\n             DESCRIPTION: Replace null values, alias for na.fill().undefinedDataFrame.fillna() and DataFrameNaFunctions.fill() are aliases of each other.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.fillna(value, subset=None)",
        "text": "fillna()"
    }, {
        "name": "sql.DataFrame.filter",
        "type": "method",
        "description": "PARAMETERS: condition\n             DESCRIPTION: Filters rows using the given condition.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.filter(condition)",
        "text": "filter()"
    }, {
        "name": "sql.DataFrame.first",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Returns the first row as a Row.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.first()",
        "text": "first()"
    }, {
        "name": "sql.DataFrame.flatMap",
        "type": "method",
        "description": "PARAMETERS: f\n             DESCRIPTION: Returns a new RDD by first applying the f function to each Row,undefinedand then flattening the results.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.flatMap(f)",
        "text": "flatMap()"
    }, {
        "name": "sql.DataFrame.foreach",
        "type": "method",
        "description": "PARAMETERS: f\n             DESCRIPTION: Applies the f function to all Row of this DataFrame.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.foreach(f)",
        "text": "foreach()"
    }, {
        "name": "sql.DataFrame.foreachPartition",
        "type": "method",
        "description": "PARAMETERS: f\n             DESCRIPTION: Applies the f function to each partition of this DataFrame.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.foreachPartition(f)",
        "text": "foreachPartition()"
    }, {
        "name": "sql.DataFrame.freqItems",
        "type": "method",
        "description": "PARAMETERS: cols, support=None\n             DESCRIPTION: Finding frequent items for columns, possibly with false positives. Using theundefinedfrequent element count algorithm described in\n&#8220;http://dx.doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou&#8221;.\nDataFrame.freqItems() and DataFrameStatFunctions.freqItems() are aliases.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.freqItems(cols, support=None)",
        "text": "freqItems()"
    }, {
        "name": "sql.DataFrame.groupBy",
        "type": "method",
        "description": "PARAMETERS: *cols\n             DESCRIPTION: Groups the DataFrame using the specified columns,undefinedso we can run aggregation on them. See GroupedData\nfor all the available aggregate functions.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.groupBy(*cols)",
        "text": "groupBy()"
    }, {
        "name": "sql.DataFrame.groupby",
        "type": "method",
        "description": "PARAMETERS: *cols\n             DESCRIPTION: Groups the DataFrame using the specified columns,undefinedso we can run aggregation on them. See GroupedData\nfor all the available aggregate functions.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.groupby(*cols)",
        "text": "groupby()"
    }, {
        "name": "sql.DataFrame.head",
        "type": "method",
        "description": "PARAMETERS: n=None\n             DESCRIPTION: Returns the first n rows.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.head(n=None)",
        "text": "head()"
    }, {
        "name": "sql.DataFrame.insertInto",
        "type": "method",
        "description": "PARAMETERS: tableName, overwrite=False\n             DESCRIPTION: Inserts the contents of this DataFrame into the specified table.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.insertInto(tableName, overwrite=False)",
        "text": "insertInto()"
    }, {
        "name": "sql.DataFrame.intersect",
        "type": "method",
        "description": "PARAMETERS: other\n             DESCRIPTION: Return a new DataFrame containing rows only inundefinedboth this frame and another frame.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.intersect(other)",
        "text": "intersect()"
    }, {
        "name": "sql.DataFrame.isLocal",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Returns True if the collect() and take() methods can be run locallyundefined(without any Spark executors).",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.isLocal()",
        "text": "isLocal()"
    }, {
        "name": "sql.DataFrame.join",
        "type": "method",
        "description": "PARAMETERS: other, on=None, how=None\n             DESCRIPTION: Joins with another DataFrame, using the given join expression.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.join(other, on=None, how=None)",
        "text": "join()"
    }, {
        "name": "sql.DataFrame.limit",
        "type": "method",
        "description": "PARAMETERS: num\n             DESCRIPTION: Limits the result count to the number specified.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.limit(num)",
        "text": "limit()"
    }, {
        "name": "sql.DataFrame.map",
        "type": "method",
        "description": "PARAMETERS: f\n             DESCRIPTION: Returns a new RDD by applying a the f function to each Row.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.map(f)",
        "text": "map()"
    }, {
        "name": "sql.DataFrame.mapPartitions",
        "type": "method",
        "description": "PARAMETERS: f, preservesPartitioning=False\n             DESCRIPTION: Returns a new RDD by applying the f function to each partition.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.mapPartitions(f, preservesPartitioning=False)",
        "text": "mapPartitions()"
    }, {
        "name": "sql.DataFrame.orderBy",
        "type": "method",
        "description": "PARAMETERS: *cols, **kwargs\n             DESCRIPTION: Returns a new DataFrame sorted by the specified column(s).",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.orderBy(*cols, **kwargs)",
        "text": "orderBy()"
    }, {
        "name": "sql.DataFrame.persist",
        "type": "method",
        "description": "PARAMETERS: storageLevel=StorageLevel(False, True, False, False, 1)\n             DESCRIPTION: Sets the storage level to persist its values across operationsundefinedafter the first time it is computed. This can only be used to assign\na new storage level if the RDD does not have a storage level set yet.\nIf no storage level is specified defaults to (MEMORY_ONLY_SER).",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.persist(storageLevel=StorageLevel(False, True, False, False, 1))",
        "text": "persist()"
    }, {
        "name": "sql.DataFrame.printSchema",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Prints out the schema in the tree format.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.printSchema()",
        "text": "printSchema()"
    }, {
        "name": "sql.DataFrame.randomSplit",
        "type": "method",
        "description": "PARAMETERS: weights, seed=None\n             DESCRIPTION: Randomly splits this DataFrame with the provided weights.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.randomSplit(weights, seed=None)",
        "text": "randomSplit()"
    }, {
        "name": "sql.DataFrame.registerAsTable",
        "type": "method",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.registerAsTable(name)",
        "text": "registerAsTable()"
    }, {
        "name": "sql.DataFrame.registerTempTable",
        "type": "method",
        "description": "PARAMETERS: name\n             DESCRIPTION: Registers this RDD as a temporary table using the given name.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.registerTempTable(name)",
        "text": "registerTempTable()"
    }, {
        "name": "sql.DataFrame.repartition",
        "type": "method",
        "description": "PARAMETERS: numPartitions, *cols\n             DESCRIPTION: Returns a new DataFrame partitioned by the given partitioning expressions. Theundefinedresulting DataFrame is hash partitioned.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.repartition(numPartitions, *cols)",
        "text": "repartition()"
    }, {
        "name": "sql.DataFrame.replace",
        "type": "method",
        "description": "PARAMETERS: to_replace, value, subset=None\n             DESCRIPTION: Returns a new DataFrame replacing a value with another value.undefinedDataFrame.replace() and DataFrameNaFunctions.replace() are\naliases of each other.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.replace(to_replace, value, subset=None)",
        "text": "replace()"
    }, {
        "name": "sql.DataFrame.rollup",
        "type": "method",
        "description": "PARAMETERS: *cols\n             DESCRIPTION: Create a multi-dimensional rollup for the current DataFrame usingundefinedthe specified columns, so we can run aggregation on them.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.rollup(*cols)",
        "text": "rollup()"
    }, {
        "name": "sql.DataFrame.sample",
        "type": "method",
        "description": "PARAMETERS: withReplacement, fraction, seed=None\n             DESCRIPTION: Returns a sampled subset of this DataFrame.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.sample(withReplacement, fraction, seed=None)",
        "text": "sample()"
    }, {
        "name": "sql.DataFrame.sampleBy",
        "type": "method",
        "description": "PARAMETERS: col, fractions, seed=None\n             DESCRIPTION: Returns a stratified sample without replacement based on theundefinedfraction given on each stratum.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.sampleBy(col, fractions, seed=None)",
        "text": "sampleBy()"
    }, {
        "name": "sql.DataFrame.save",
        "type": "method",
        "description": "PARAMETERS: path=None, source=None, mode='error', **options\n             DESCRIPTION: Saves the contents of the DataFrame to a data source.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.save(path=None, source=None, mode='error', **options)",
        "text": "save()"
    }, {
        "name": "sql.DataFrame.saveAsParquetFile",
        "type": "method",
        "description": "PARAMETERS: path\n             DESCRIPTION: Saves the contents as a Parquet file, preserving the schema.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.saveAsParquetFile(path)",
        "text": "saveAsParquetFile()"
    }, {
        "name": "sql.DataFrame.saveAsTable",
        "type": "method",
        "description": "PARAMETERS: tableName, source=None, mode='error', **options\n             DESCRIPTION: Saves the contents of this DataFrame to a data source as a table.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.saveAsTable(tableName, source=None, mode='error', **options)",
        "text": "saveAsTable()"
    }, {
        "name": "sql.DataFrame.select",
        "type": "method",
        "description": "PARAMETERS: *cols\n             DESCRIPTION: Projects a set of expressions and returns a new DataFrame.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.select(*cols)",
        "text": "select()"
    }, {
        "name": "sql.DataFrame.selectExpr",
        "type": "method",
        "description": "PARAMETERS: *expr\n             DESCRIPTION: Projects a set of SQL expressions and returns a new DataFrame.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.selectExpr(*expr)",
        "text": "selectExpr()"
    }, {
        "name": "sql.DataFrame.show",
        "type": "method",
        "description": "PARAMETERS: n=20, truncate=True\n             DESCRIPTION: Prints the first n rows to the console.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.show(n=20, truncate=True)",
        "text": "show()"
    }, {
        "name": "sql.DataFrame.sort",
        "type": "method",
        "description": "PARAMETERS: *cols, **kwargs\n             DESCRIPTION: Returns a new DataFrame sorted by the specified column(s).",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.sort(*cols, **kwargs)",
        "text": "sort()"
    }, {
        "name": "sql.DataFrame.sortWithinPartitions",
        "type": "method",
        "description": "PARAMETERS: *cols, **kwargs\n             DESCRIPTION: Returns a new DataFrame with each partition sorted by the specified column(s).",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.sortWithinPartitions(*cols, **kwargs)",
        "text": "sortWithinPartitions()"
    }, {
        "name": "sql.DataFrame.subtract",
        "type": "method",
        "description": "PARAMETERS: other\n             DESCRIPTION: Return a new DataFrame containing rows in this frameundefinedbut not in another frame.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.subtract(other)",
        "text": "subtract()"
    }, {
        "name": "sql.DataFrame.take",
        "type": "method",
        "description": "PARAMETERS: num\n             DESCRIPTION: Returns the first num rows as a list of Row.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.take(num)",
        "text": "take()"
    }, {
        "name": "sql.DataFrame.toDF",
        "type": "method",
        "description": "PARAMETERS: *cols\n             DESCRIPTION: Returns a new class:DataFrame that with new specified column names",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.toDF(*cols)",
        "text": "toDF()"
    }, {
        "name": "sql.DataFrame.toJSON",
        "type": "method",
        "description": "PARAMETERS: use_unicode=True\n             DESCRIPTION: Converts a DataFrame into a RDD of string.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.toJSON(use_unicode=True)",
        "text": "toJSON()"
    }, {
        "name": "sql.DataFrame.toPandas",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Returns the contents of this DataFrame as Pandas pandas.DataFrame.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.toPandas()",
        "text": "toPandas()"
    }, {
        "name": "sql.DataFrame.unionAll",
        "type": "method",
        "description": "PARAMETERS: other\n             DESCRIPTION: Return a new DataFrame containing union of rows in thisundefinedframe and another frame.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.unionAll(other)",
        "text": "unionAll()"
    }, {
        "name": "sql.DataFrame.unpersist",
        "type": "method",
        "description": "PARAMETERS: blocking=True\n             DESCRIPTION: Marks the DataFrame as non-persistent, and remove all blocks for it fromundefinedmemory and disk.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.unpersist(blocking=True)",
        "text": "unpersist()"
    }, {
        "name": "sql.DataFrame.where",
        "type": "method",
        "description": "PARAMETERS: condition\n             DESCRIPTION: Filters rows using the given condition.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.where(condition)",
        "text": "where()"
    }, {
        "name": "sql.DataFrame.withColumn",
        "type": "method",
        "description": "PARAMETERS: colName, col\n             DESCRIPTION: Returns a new DataFrame by adding a column or replacing theundefinedexisting column that has the same name.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.withColumn(colName, col)",
        "text": "withColumn()"
    }, {
        "name": "sql.DataFrame.withColumnRenamed",
        "type": "method",
        "description": "PARAMETERS: existing, new\n             DESCRIPTION: Returns a new DataFrame by renaming an existing column.",
        "leftLabel": "sql.DataFrame",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrame.withColumnRenamed(existing, new)",
        "text": "withColumnRenamed()"
    }, {
        "name": "sql.GroupedData",
        "type": "class",
        "description": "PARAMETERS: jdf, sql_ctx\n             DESCRIPTION: A set of methods for aggregations on a DataFrame,undefinedcreated by DataFrame.groupBy().",
        "rightLabel": "class | PySpark",
        "displayText": "sql.GroupedData(jdf, sql_ctx)",
        "text": "GroupedData()",
        "leftLabel": "sql.GroupedData"
    }, {
        "name": "sql.GroupedData.agg",
        "type": "method",
        "description": "PARAMETERS: *exprs\n             DESCRIPTION: Compute aggregates and returns the result as a DataFrame.",
        "leftLabel": "sql.GroupedData",
        "rightLabel": "method | PySpark",
        "displayText": "sql.GroupedData.agg(*exprs)",
        "text": "agg()"
    }, {
        "name": "sql.GroupedData.avg",
        "type": "method",
        "description": "PARAMETERS: *args\n             DESCRIPTION: Computes average values for each numeric columns for each group.",
        "leftLabel": "sql.GroupedData",
        "rightLabel": "method | PySpark",
        "displayText": "sql.GroupedData.avg(*args)",
        "text": "avg()"
    }, {
        "name": "sql.GroupedData.count",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Counts the number of records for each group.",
        "leftLabel": "sql.GroupedData",
        "rightLabel": "method | PySpark",
        "displayText": "sql.GroupedData.count()",
        "text": "count()"
    }, {
        "name": "sql.GroupedData.max",
        "type": "method",
        "description": "PARAMETERS: *args\n             DESCRIPTION: Computes the max value for each numeric columns for each group.",
        "leftLabel": "sql.GroupedData",
        "rightLabel": "method | PySpark",
        "displayText": "sql.GroupedData.max(*args)",
        "text": "max()"
    }, {
        "name": "sql.GroupedData.mean",
        "type": "method",
        "description": "PARAMETERS: *args\n             DESCRIPTION: Computes average values for each numeric columns for each group.",
        "leftLabel": "sql.GroupedData",
        "rightLabel": "method | PySpark",
        "displayText": "sql.GroupedData.mean(*args)",
        "text": "mean()"
    }, {
        "name": "sql.GroupedData.min",
        "type": "method",
        "description": "PARAMETERS: *args\n             DESCRIPTION: Computes the min value for each numeric column for each group.",
        "leftLabel": "sql.GroupedData",
        "rightLabel": "method | PySpark",
        "displayText": "sql.GroupedData.min(*args)",
        "text": "min()"
    }, {
        "name": "sql.GroupedData.pivot",
        "type": "method",
        "description": "PARAMETERS: pivot_col, values=None\n             DESCRIPTION: Pivots a column of the current [[DataFrame]] and perform the specified aggregation.undefinedThere are two versions of pivot function: one that requires the caller to specify the list\nof distinct values to pivot on, and one that does not. The latter is more concise but less\nefficient, because Spark needs to first compute the list of distinct values internally.",
        "leftLabel": "sql.GroupedData",
        "rightLabel": "method | PySpark",
        "displayText": "sql.GroupedData.pivot(pivot_col, values=None)",
        "text": "pivot()"
    }, {
        "name": "sql.GroupedData.sum",
        "type": "method",
        "description": "PARAMETERS: *args\n             DESCRIPTION: Compute the sum for each numeric columns for each group.",
        "leftLabel": "sql.GroupedData",
        "rightLabel": "method | PySpark",
        "displayText": "sql.GroupedData.sum(*args)",
        "text": "sum()"
    }, {
        "name": "sql.Column",
        "type": "class",
        "description": "PARAMETERS: jc\n             DESCRIPTION: A column in a DataFrame.",
        "rightLabel": "class | PySpark",
        "displayText": "sql.Column(jc)",
        "text": "Column()",
        "leftLabel": "sql.Column"
    }, {
        "name": "sql.Column.alias",
        "type": "method",
        "description": "PARAMETERS: *alias\n             DESCRIPTION: Returns this column aliased with a new name or names (in the case of expressions thatundefinedreturn more than one column, such as explode).",
        "leftLabel": "sql.Column",
        "rightLabel": "method | PySpark",
        "displayText": "sql.Column.alias(*alias)",
        "text": "alias()"
    }, {
        "name": "sql.Column.asc",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Returns a sort expression based on the ascending order of the given column name.",
        "leftLabel": "sql.Column",
        "rightLabel": "method | PySpark",
        "displayText": "sql.Column.asc()",
        "text": "asc()"
    }, {
        "name": "sql.Column.astype",
        "type": "method",
        "description": "PARAMETERS: dataType\n             DESCRIPTION: Convert the column into type dataType.",
        "leftLabel": "sql.Column",
        "rightLabel": "method | PySpark",
        "displayText": "sql.Column.astype(dataType)",
        "text": "astype()"
    }, {
        "name": "sql.Column.between",
        "type": "method",
        "description": "PARAMETERS: lowerBound, upperBound\n             DESCRIPTION: A boolean expression that is evaluated to true if the value of thisundefinedexpression is between the given columns.",
        "leftLabel": "sql.Column",
        "rightLabel": "method | PySpark",
        "displayText": "sql.Column.between(lowerBound, upperBound)",
        "text": "between()"
    }, {
        "name": "sql.Column.bitwiseAND",
        "type": "method",
        "description": "PARAMETERS: other\n             DESCRIPTION: binary operator",
        "leftLabel": "sql.Column",
        "rightLabel": "method | PySpark",
        "displayText": "sql.Column.bitwiseAND(other)",
        "text": "bitwiseAND()"
    }, {
        "name": "sql.Column.bitwiseOR",
        "type": "method",
        "description": "PARAMETERS: other\n             DESCRIPTION: binary operator",
        "leftLabel": "sql.Column",
        "rightLabel": "method | PySpark",
        "displayText": "sql.Column.bitwiseOR(other)",
        "text": "bitwiseOR()"
    }, {
        "name": "sql.Column.bitwiseXOR",
        "type": "method",
        "description": "PARAMETERS: other\n             DESCRIPTION: binary operator",
        "leftLabel": "sql.Column",
        "rightLabel": "method | PySpark",
        "displayText": "sql.Column.bitwiseXOR(other)",
        "text": "bitwiseXOR()"
    }, {
        "name": "sql.Column.cast",
        "type": "method",
        "description": "PARAMETERS: dataType\n             DESCRIPTION: Convert the column into type dataType.",
        "leftLabel": "sql.Column",
        "rightLabel": "method | PySpark",
        "displayText": "sql.Column.cast(dataType)",
        "text": "cast()"
    }, {
        "name": "sql.Column.desc",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Returns a sort expression based on the descending order of the given column name.",
        "leftLabel": "sql.Column",
        "rightLabel": "method | PySpark",
        "displayText": "sql.Column.desc()",
        "text": "desc()"
    }, {
        "name": "sql.Column.endswith",
        "type": "method",
        "description": "PARAMETERS: other\n             DESCRIPTION: binary operator",
        "leftLabel": "sql.Column",
        "rightLabel": "method | PySpark",
        "displayText": "sql.Column.endswith(other)",
        "text": "endswith()"
    }, {
        "name": "sql.Column.getField",
        "type": "method",
        "description": "PARAMETERS: name\n             DESCRIPTION: An expression that gets a field by name in a StructField.",
        "leftLabel": "sql.Column",
        "rightLabel": "method | PySpark",
        "displayText": "sql.Column.getField(name)",
        "text": "getField()"
    }, {
        "name": "sql.Column.getItem",
        "type": "method",
        "description": "PARAMETERS: key\n             DESCRIPTION: An expression that gets an item at position ordinal out of a list,undefinedor gets an item by key out of a dict.",
        "leftLabel": "sql.Column",
        "rightLabel": "method | PySpark",
        "displayText": "sql.Column.getItem(key)",
        "text": "getItem()"
    }, {
        "name": "sql.Column.inSet",
        "type": "method",
        "description": "PARAMETERS: *cols\n             DESCRIPTION: A boolean expression that is evaluated to true if the value of thisundefinedexpression is contained by the evaluated values of the arguments.",
        "leftLabel": "sql.Column",
        "rightLabel": "method | PySpark",
        "displayText": "sql.Column.inSet(*cols)",
        "text": "inSet()"
    }, {
        "name": "sql.Column.isNotNull",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: True if the current expression is not null.",
        "leftLabel": "sql.Column",
        "rightLabel": "method | PySpark",
        "displayText": "sql.Column.isNotNull()",
        "text": "isNotNull()"
    }, {
        "name": "sql.Column.isNull",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: True if the current expression is null.",
        "leftLabel": "sql.Column",
        "rightLabel": "method | PySpark",
        "displayText": "sql.Column.isNull()",
        "text": "isNull()"
    }, {
        "name": "sql.Column.isin",
        "type": "method",
        "description": "PARAMETERS: *cols\n             DESCRIPTION: A boolean expression that is evaluated to true if the value of thisundefinedexpression is contained by the evaluated values of the arguments.",
        "leftLabel": "sql.Column",
        "rightLabel": "method | PySpark",
        "displayText": "sql.Column.isin(*cols)",
        "text": "isin()"
    }, {
        "name": "sql.Column.like",
        "type": "method",
        "description": "PARAMETERS: other\n             DESCRIPTION: binary operator",
        "leftLabel": "sql.Column",
        "rightLabel": "method | PySpark",
        "displayText": "sql.Column.like(other)",
        "text": "like()"
    }, {
        "name": "sql.Column.otherwise",
        "type": "method",
        "description": "PARAMETERS: value\n             DESCRIPTION: Evaluates a list of conditions and returns one of multiple possible result expressions.undefinedIf Column.otherwise() is not invoked, None is returned for unmatched conditions.",
        "leftLabel": "sql.Column",
        "rightLabel": "method | PySpark",
        "displayText": "sql.Column.otherwise(value)",
        "text": "otherwise()"
    }, {
        "name": "sql.Column.over",
        "type": "method",
        "description": "PARAMETERS: window\n             DESCRIPTION: Define a windowing column.",
        "leftLabel": "sql.Column",
        "rightLabel": "method | PySpark",
        "displayText": "sql.Column.over(window)",
        "text": "over()"
    }, {
        "name": "sql.Column.rlike",
        "type": "method",
        "description": "PARAMETERS: other\n             DESCRIPTION: binary operator",
        "leftLabel": "sql.Column",
        "rightLabel": "method | PySpark",
        "displayText": "sql.Column.rlike(other)",
        "text": "rlike()"
    }, {
        "name": "sql.Column.startswith",
        "type": "method",
        "description": "PARAMETERS: other\n             DESCRIPTION: binary operator",
        "leftLabel": "sql.Column",
        "rightLabel": "method | PySpark",
        "displayText": "sql.Column.startswith(other)",
        "text": "startswith()"
    }, {
        "name": "sql.Column.substr",
        "type": "method",
        "description": "PARAMETERS: startPos, length\n             DESCRIPTION: Return a Column which is a substring of the column.",
        "leftLabel": "sql.Column",
        "rightLabel": "method | PySpark",
        "displayText": "sql.Column.substr(startPos, length)",
        "text": "substr()"
    }, {
        "name": "sql.Column.when",
        "type": "method",
        "description": "PARAMETERS: condition, value\n             DESCRIPTION: Evaluates a list of conditions and returns one of multiple possible result expressions.undefinedIf Column.otherwise() is not invoked, None is returned for unmatched conditions.",
        "leftLabel": "sql.Column",
        "rightLabel": "method | PySpark",
        "displayText": "sql.Column.when(condition, value)",
        "text": "when()"
    }, {
        "name": "sql.Row",
        "type": "class",
        "description": "PARAMETERS: \n             DESCRIPTION: A row in DataFrame. The fields in it can be accessed like attributes.",
        "rightLabel": "class | PySpark",
        "displayText": "sql.Row()",
        "text": "Row()",
        "leftLabel": "sql.Row"
    }, {
        "name": "sql.Row.asDict",
        "type": "method",
        "description": "PARAMETERS: recursive=False\n             DESCRIPTION: Return as an dict",
        "leftLabel": "sql.Row",
        "rightLabel": "method | PySpark",
        "displayText": "sql.Row.asDict(recursive=False)",
        "text": "asDict()"
    }, {
        "name": "sql.DataFrameNaFunctions",
        "type": "class",
        "description": "PARAMETERS: df\n             DESCRIPTION: Functionality for working with missing data in DataFrame.",
        "rightLabel": "class | PySpark",
        "displayText": "sql.DataFrameNaFunctions(df)",
        "text": "DataFrameNaFunctions()",
        "leftLabel": "sql.DataFrameNaFunctions"
    }, {
        "name": "sql.DataFrameNaFunctions.drop",
        "type": "method",
        "description": "PARAMETERS: how='any', thresh=None, subset=None\n             DESCRIPTION: Returns a new DataFrame omitting rows with null values.undefinedDataFrame.dropna() and DataFrameNaFunctions.drop() are aliases of each other.",
        "leftLabel": "sql.DataFrameNaFunctions",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrameNaFunctions.drop(how='any', thresh=None, subset=None)",
        "text": "drop()"
    }, {
        "name": "sql.DataFrameNaFunctions.fill",
        "type": "method",
        "description": "PARAMETERS: value, subset=None\n             DESCRIPTION: Replace null values, alias for na.fill().undefinedDataFrame.fillna() and DataFrameNaFunctions.fill() are aliases of each other.",
        "leftLabel": "sql.DataFrameNaFunctions",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrameNaFunctions.fill(value, subset=None)",
        "text": "fill()"
    }, {
        "name": "sql.DataFrameNaFunctions.replace",
        "type": "method",
        "description": "PARAMETERS: to_replace, value, subset=None\n             DESCRIPTION: Returns a new DataFrame replacing a value with another value.undefinedDataFrame.replace() and DataFrameNaFunctions.replace() are\naliases of each other.",
        "leftLabel": "sql.DataFrameNaFunctions",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrameNaFunctions.replace(to_replace, value, subset=None)",
        "text": "replace()"
    }, {
        "name": "sql.DataFrameStatFunctions",
        "type": "class",
        "description": "PARAMETERS: df\n             DESCRIPTION: Functionality for statistic functions with DataFrame.",
        "rightLabel": "class | PySpark",
        "displayText": "sql.DataFrameStatFunctions(df)",
        "text": "DataFrameStatFunctions()",
        "leftLabel": "sql.DataFrameStatFunctions"
    }, {
        "name": "sql.DataFrameStatFunctions.corr",
        "type": "method",
        "description": "PARAMETERS: col1, col2, method=None\n             DESCRIPTION: Calculates the correlation of two columns of a DataFrame as a double value.undefinedCurrently only supports the Pearson Correlation Coefficient.\nDataFrame.corr() and DataFrameStatFunctions.corr() are aliases of each other.",
        "leftLabel": "sql.DataFrameStatFunctions",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrameStatFunctions.corr(col1, col2, method=None)",
        "text": "corr()"
    }, {
        "name": "sql.DataFrameStatFunctions.cov",
        "type": "method",
        "description": "PARAMETERS: col1, col2\n             DESCRIPTION: Calculate the sample covariance for the given columns, specified by their names, as aundefineddouble value. DataFrame.cov() and DataFrameStatFunctions.cov() are aliases.",
        "leftLabel": "sql.DataFrameStatFunctions",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrameStatFunctions.cov(col1, col2)",
        "text": "cov()"
    }, {
        "name": "sql.DataFrameStatFunctions.crosstab",
        "type": "method",
        "description": "PARAMETERS: col1, col2\n             DESCRIPTION: Computes a pair-wise frequency table of the given columns. Also known as a contingencyundefinedtable. The number of distinct values for each column should be less than 1e4. At most 1e6\nnon-zero pair frequencies will be returned.\nThe first column of each row will be the distinct values of col1 and the column names\nwill be the distinct values of col2. The name of the first column will be $col1_$col2.\nPairs that have no occurrences will have zero as their counts.\nDataFrame.crosstab() and DataFrameStatFunctions.crosstab() are aliases.",
        "leftLabel": "sql.DataFrameStatFunctions",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrameStatFunctions.crosstab(col1, col2)",
        "text": "crosstab()"
    }, {
        "name": "sql.DataFrameStatFunctions.freqItems",
        "type": "method",
        "description": "PARAMETERS: cols, support=None\n             DESCRIPTION: Finding frequent items for columns, possibly with false positives. Using theundefinedfrequent element count algorithm described in\n&#8220;http://dx.doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou&#8221;.\nDataFrame.freqItems() and DataFrameStatFunctions.freqItems() are aliases.",
        "leftLabel": "sql.DataFrameStatFunctions",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrameStatFunctions.freqItems(cols, support=None)",
        "text": "freqItems()"
    }, {
        "name": "sql.DataFrameStatFunctions.sampleBy",
        "type": "method",
        "description": "PARAMETERS: col, fractions, seed=None\n             DESCRIPTION: Returns a stratified sample without replacement based on theundefinedfraction given on each stratum.",
        "leftLabel": "sql.DataFrameStatFunctions",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrameStatFunctions.sampleBy(col, fractions, seed=None)",
        "text": "sampleBy()"
    }, {
        "name": "sql.Window",
        "type": "class",
        "description": "PARAMETERS: *cols\n             DESCRIPTION: Utility functions for defining window in DataFrames.",
        "rightLabel": "class | PySpark",
        "displayText": "sql.Window(*cols)",
        "text": "Window()",
        "leftLabel": "sql.Window"
    }, {
        "name": "sql.WindowSpec",
        "type": "class",
        "description": "PARAMETERS: jspec\n             DESCRIPTION: A window specification that defines the partitioning, ordering,undefinedand frame boundaries.",
        "rightLabel": "class | PySpark",
        "displayText": "sql.WindowSpec(jspec)",
        "text": "WindowSpec()",
        "leftLabel": "sql.WindowSpec"
    }, {
        "name": "sql.WindowSpec.orderBy",
        "type": "method",
        "description": "PARAMETERS: *cols\n             DESCRIPTION: Defines the ordering columns in a WindowSpec.",
        "leftLabel": "sql.WindowSpec",
        "rightLabel": "method | PySpark",
        "displayText": "sql.WindowSpec.orderBy(*cols)",
        "text": "orderBy()"
    }, {
        "name": "sql.WindowSpec.partitionBy",
        "type": "method",
        "description": "PARAMETERS: *cols\n             DESCRIPTION: Defines the partitioning columns in a WindowSpec.",
        "leftLabel": "sql.WindowSpec",
        "rightLabel": "method | PySpark",
        "displayText": "sql.WindowSpec.partitionBy(*cols)",
        "text": "partitionBy()"
    }, {
        "name": "sql.WindowSpec.rangeBetween",
        "type": "method",
        "description": "PARAMETERS: start, end\n             DESCRIPTION: Defines the frame boundaries, from start (inclusive) to end (inclusive).",
        "leftLabel": "sql.WindowSpec",
        "rightLabel": "method | PySpark",
        "displayText": "sql.WindowSpec.rangeBetween(start, end)",
        "text": "rangeBetween()"
    }, {
        "name": "sql.WindowSpec.rowsBetween",
        "type": "method",
        "description": "PARAMETERS: start, end\n             DESCRIPTION: Defines the frame boundaries, from start (inclusive) to end (inclusive).",
        "leftLabel": "sql.WindowSpec",
        "rightLabel": "method | PySpark",
        "displayText": "sql.WindowSpec.rowsBetween(start, end)",
        "text": "rowsBetween()"
    }, {
        "name": "sql.DataFrameReader",
        "type": "class",
        "description": "PARAMETERS: sqlContext\n             DESCRIPTION: Interface used to load a DataFrame from external storage systemsundefined(e.g. file systems, key-value stores, etc). Use SQLContext.read()\nto access this.",
        "rightLabel": "class | PySpark",
        "displayText": "sql.DataFrameReader(sqlContext)",
        "text": "DataFrameReader()",
        "leftLabel": "sql.DataFrameReader"
    }, {
        "name": "sql.DataFrameReader.format",
        "type": "method",
        "description": "PARAMETERS: source\n             DESCRIPTION: Specifies the input data source format.",
        "leftLabel": "sql.DataFrameReader",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrameReader.format(source)",
        "text": "format()"
    }, {
        "name": "sql.DataFrameReader.jdbc",
        "type": "method",
        "description": "PARAMETERS: url, table, column=None, lowerBound=None, upperBound=None, numPartitions=None, predicates=None, properties=None\n             DESCRIPTION: Construct a DataFrame representing the database table accessibleundefinedvia JDBC URL url named table and connection properties.",
        "leftLabel": "sql.DataFrameReader",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrameReader.jdbc(url, table, column=None, lowerBound=None, upperBound=None, numPartitions=None, predicates=None, properties=None)",
        "text": "jdbc()"
    }, {
        "name": "sql.DataFrameReader.json",
        "type": "method",
        "description": "PARAMETERS: path, schema=None\n             DESCRIPTION: Loads a JSON file (one object per line) or an RDD of Strings storing JSON objectsundefined(one object per record) and returns the result as a :class`DataFrame`.",
        "leftLabel": "sql.DataFrameReader",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrameReader.json(path, schema=None)",
        "text": "json()"
    }, {
        "name": "sql.DataFrameReader.load",
        "type": "method",
        "description": "PARAMETERS: path=None, format=None, schema=None, **options\n             DESCRIPTION: Loads data from a data source and returns it as a :class`DataFrame`.",
        "leftLabel": "sql.DataFrameReader",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrameReader.load(path=None, format=None, schema=None, **options)",
        "text": "load()"
    }, {
        "name": "sql.DataFrameReader.option",
        "type": "method",
        "description": "PARAMETERS: key, value\n             DESCRIPTION: Adds an input option for the underlying data source.",
        "leftLabel": "sql.DataFrameReader",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrameReader.option(key, value)",
        "text": "option()"
    }, {
        "name": "sql.DataFrameReader.options",
        "type": "method",
        "description": "PARAMETERS: **options\n             DESCRIPTION: Adds input options for the underlying data source.",
        "leftLabel": "sql.DataFrameReader",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrameReader.options(**options)",
        "text": "options()"
    }, {
        "name": "sql.DataFrameReader.orc",
        "type": "method",
        "description": "PARAMETERS: path\n             DESCRIPTION: Loads an ORC file, returning the result as a DataFrame.",
        "leftLabel": "sql.DataFrameReader",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrameReader.orc(path)",
        "text": "orc()"
    }, {
        "name": "sql.DataFrameReader.parquet",
        "type": "method",
        "description": "PARAMETERS: *paths\n             DESCRIPTION: Loads a Parquet file, returning the result as a DataFrame.",
        "leftLabel": "sql.DataFrameReader",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrameReader.parquet(*paths)",
        "text": "parquet()"
    }, {
        "name": "sql.DataFrameReader.schema",
        "type": "method",
        "description": "PARAMETERS: schema\n             DESCRIPTION: Specifies the input schema.",
        "leftLabel": "sql.DataFrameReader",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrameReader.schema(schema)",
        "text": "schema()"
    }, {
        "name": "sql.DataFrameReader.table",
        "type": "method",
        "description": "PARAMETERS: tableName\n             DESCRIPTION: Returns the specified table as a DataFrame.",
        "leftLabel": "sql.DataFrameReader",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrameReader.table(tableName)",
        "text": "table()"
    }, {
        "name": "sql.DataFrameReader.text",
        "type": "method",
        "description": "PARAMETERS: paths\n             DESCRIPTION: Loads a text file and returns a [[DataFrame]] with a single string column named &#8220;value&#8221;.",
        "leftLabel": "sql.DataFrameReader",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrameReader.text(paths)",
        "text": "text()"
    }, {
        "name": "sql.DataFrameWriter",
        "type": "class",
        "description": "PARAMETERS: df\n             DESCRIPTION: Interface used to write a [[DataFrame]] to external storage systemsundefined(e.g. file systems, key-value stores, etc). Use DataFrame.write()\nto access this.",
        "rightLabel": "class | PySpark",
        "displayText": "sql.DataFrameWriter(df)",
        "text": "DataFrameWriter()",
        "leftLabel": "sql.DataFrameWriter"
    }, {
        "name": "sql.DataFrameWriter.format",
        "type": "method",
        "description": "PARAMETERS: source\n             DESCRIPTION: Specifies the underlying output data source.",
        "leftLabel": "sql.DataFrameWriter",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrameWriter.format(source)",
        "text": "format()"
    }, {
        "name": "sql.DataFrameWriter.insertInto",
        "type": "method",
        "description": "PARAMETERS: tableName, overwrite=False\n             DESCRIPTION: Inserts the content of the DataFrame to the specified table.",
        "leftLabel": "sql.DataFrameWriter",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrameWriter.insertInto(tableName, overwrite=False)",
        "text": "insertInto()"
    }, {
        "name": "sql.DataFrameWriter.jdbc",
        "type": "method",
        "description": "PARAMETERS: url, table, mode=None, properties=None\n             DESCRIPTION: Saves the content of the DataFrame to a external database table via JDBC.",
        "leftLabel": "sql.DataFrameWriter",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrameWriter.jdbc(url, table, mode=None, properties=None)",
        "text": "jdbc()"
    }, {
        "name": "sql.DataFrameWriter.json",
        "type": "method",
        "description": "PARAMETERS: path, mode=None\n             DESCRIPTION: Saves the content of the DataFrame in JSON format at the specified path.",
        "leftLabel": "sql.DataFrameWriter",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrameWriter.json(path, mode=None)",
        "text": "json()"
    }, {
        "name": "sql.DataFrameWriter.mode",
        "type": "method",
        "description": "PARAMETERS: saveMode\n             DESCRIPTION: Specifies the behavior when data or table already exists.",
        "leftLabel": "sql.DataFrameWriter",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrameWriter.mode(saveMode)",
        "text": "mode()"
    }, {
        "name": "sql.DataFrameWriter.option",
        "type": "method",
        "description": "PARAMETERS: key, value\n             DESCRIPTION: Adds an output option for the underlying data source.",
        "leftLabel": "sql.DataFrameWriter",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrameWriter.option(key, value)",
        "text": "option()"
    }, {
        "name": "sql.DataFrameWriter.options",
        "type": "method",
        "description": "PARAMETERS: **options\n             DESCRIPTION: Adds output options for the underlying data source.",
        "leftLabel": "sql.DataFrameWriter",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrameWriter.options(**options)",
        "text": "options()"
    }, {
        "name": "sql.DataFrameWriter.orc",
        "type": "method",
        "description": "PARAMETERS: path, mode=None, partitionBy=None\n             DESCRIPTION: Saves the content of the DataFrame in ORC format at the specified path.",
        "leftLabel": "sql.DataFrameWriter",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrameWriter.orc(path, mode=None, partitionBy=None)",
        "text": "orc()"
    }, {
        "name": "sql.DataFrameWriter.parquet",
        "type": "method",
        "description": "PARAMETERS: path, mode=None, partitionBy=None\n             DESCRIPTION: Saves the content of the DataFrame in Parquet format at the specified path.",
        "leftLabel": "sql.DataFrameWriter",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrameWriter.parquet(path, mode=None, partitionBy=None)",
        "text": "parquet()"
    }, {
        "name": "sql.DataFrameWriter.partitionBy",
        "type": "method",
        "description": "PARAMETERS: *cols\n             DESCRIPTION: Partitions the output by the given columns on the file system.",
        "leftLabel": "sql.DataFrameWriter",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrameWriter.partitionBy(*cols)",
        "text": "partitionBy()"
    }, {
        "name": "sql.DataFrameWriter.save",
        "type": "method",
        "description": "PARAMETERS: path=None, format=None, mode=None, partitionBy=None, **options\n             DESCRIPTION: Saves the contents of the DataFrame to a data source.",
        "leftLabel": "sql.DataFrameWriter",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrameWriter.save(path=None, format=None, mode=None, partitionBy=None, **options)",
        "text": "save()"
    }, {
        "name": "sql.DataFrameWriter.saveAsTable",
        "type": "method",
        "description": "PARAMETERS: name, format=None, mode=None, partitionBy=None, **options\n             DESCRIPTION: Saves the content of the DataFrame as the specified table.",
        "leftLabel": "sql.DataFrameWriter",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrameWriter.saveAsTable(name, format=None, mode=None, partitionBy=None, **options)",
        "text": "saveAsTable()"
    }, {
        "name": "sql.DataFrameWriter.text",
        "type": "method",
        "description": "PARAMETERS: path\n             DESCRIPTION: Saves the content of the DataFrame in a text file at the specified path.",
        "leftLabel": "sql.DataFrameWriter",
        "rightLabel": "method | PySpark",
        "displayText": "sql.DataFrameWriter.text(path)",
        "text": "text()"
    }, {
        "name": "sql.types.DataType",
        "type": "class",
        "description": "PARAMETERS: \n             DESCRIPTION: Base class for data types.",
        "rightLabel": "class | PySpark",
        "displayText": "sql.types.DataType()",
        "text": "DataType()",
        "leftLabel": "sql.types.DataType"
    }, {
        "name": "sql.types.DataType.fromInternal",
        "type": "method",
        "description": "PARAMETERS: obj\n             DESCRIPTION: Converts an internal SQL object into a native Python object.",
        "leftLabel": "sql.types.DataType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.DataType.fromInternal(obj)",
        "text": "fromInternal()"
    }, {
        "name": "sql.types.DataType.json",
        "type": "method",
        "leftLabel": "sql.types.DataType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.DataType.json()",
        "text": "json()"
    }, {
        "name": "sql.types.DataType.jsonValue",
        "type": "method",
        "leftLabel": "sql.types.DataType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.DataType.jsonValue()",
        "text": "jsonValue()"
    }, {
        "name": "sql.types.DataType.needConversion",
        "type": "method",
        "description": "PARAMETERS: \n             DESCRIPTION: Does this type need to conversion between Python object and internal SQL object.",
        "leftLabel": "sql.types.DataType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.DataType.needConversion()",
        "text": "needConversion()"
    }, {
        "name": "sql.types.DataType.simpleString",
        "type": "method",
        "leftLabel": "sql.types.DataType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.DataType.simpleString()",
        "text": "simpleString()"
    }, {
        "name": "sql.types.DataType.toInternal",
        "type": "method",
        "description": "PARAMETERS: obj\n             DESCRIPTION: Converts a Python object into an internal SQL object.",
        "leftLabel": "sql.types.DataType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.DataType.toInternal(obj)",
        "text": "toInternal()"
    }, {
        "name": "sql.types.NullType",
        "type": "class",
        "description": "PARAMETERS: \n             DESCRIPTION: Null type.",
        "rightLabel": "class | PySpark",
        "displayText": "sql.types.NullType()",
        "text": "NullType()",
        "leftLabel": "sql.types.NullType"
    }, {
        "name": "sql.types.StringType",
        "type": "class",
        "description": "PARAMETERS: \n             DESCRIPTION: String data type.",
        "rightLabel": "class | PySpark",
        "displayText": "sql.types.StringType()",
        "text": "StringType()",
        "leftLabel": "sql.types.StringType"
    }, {
        "name": "sql.types.BinaryType",
        "type": "class",
        "description": "PARAMETERS: \n             DESCRIPTION: Binary (byte array) data type.",
        "rightLabel": "class | PySpark",
        "displayText": "sql.types.BinaryType()",
        "text": "BinaryType()",
        "leftLabel": "sql.types.BinaryType"
    }, {
        "name": "sql.types.BooleanType",
        "type": "class",
        "description": "PARAMETERS: \n             DESCRIPTION: Boolean data type.",
        "rightLabel": "class | PySpark",
        "displayText": "sql.types.BooleanType()",
        "text": "BooleanType()",
        "leftLabel": "sql.types.BooleanType"
    }, {
        "name": "sql.types.DateType",
        "type": "class",
        "description": "PARAMETERS: \n             DESCRIPTION: Date (datetime.date) data type.",
        "rightLabel": "class | PySpark",
        "displayText": "sql.types.DateType()",
        "text": "DateType()",
        "leftLabel": "sql.types.DateType"
    }, {
        "name": "sql.types.DateType.EPOCH_ORDINAL",
        "type": "value",
        "leftLabel": "sql.types.DateType",
        "rightLabel": "attribute | PySpark",
        "displayText": "sql.types.DateType.EPOCH_ORDINAL",
        "text": "EPOCH_ORDINAL"
    }, {
        "name": "sql.types.DateType.fromInternal",
        "type": "method",
        "leftLabel": "sql.types.DateType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.DateType.fromInternal(v)",
        "text": "fromInternal()"
    }, {
        "name": "sql.types.DateType.needConversion",
        "type": "method",
        "leftLabel": "sql.types.DateType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.DateType.needConversion()",
        "text": "needConversion()"
    }, {
        "name": "sql.types.DateType.toInternal",
        "type": "method",
        "leftLabel": "sql.types.DateType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.DateType.toInternal(d)",
        "text": "toInternal()"
    }, {
        "name": "sql.types.TimestampType",
        "type": "class",
        "description": "PARAMETERS: \n             DESCRIPTION: Timestamp (datetime.datetime) data type.",
        "rightLabel": "class | PySpark",
        "displayText": "sql.types.TimestampType()",
        "text": "TimestampType()",
        "leftLabel": "sql.types.TimestampType"
    }, {
        "name": "sql.types.TimestampType.fromInternal",
        "type": "method",
        "leftLabel": "sql.types.TimestampType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.TimestampType.fromInternal(ts)",
        "text": "fromInternal()"
    }, {
        "name": "sql.types.TimestampType.needConversion",
        "type": "method",
        "leftLabel": "sql.types.TimestampType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.TimestampType.needConversion()",
        "text": "needConversion()"
    }, {
        "name": "sql.types.TimestampType.toInternal",
        "type": "method",
        "leftLabel": "sql.types.TimestampType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.TimestampType.toInternal(dt)",
        "text": "toInternal()"
    }, {
        "name": "sql.types.DecimalType",
        "type": "class",
        "description": "PARAMETERS: precision=10, scale=0\n             DESCRIPTION: Decimal (decimal.Decimal) data type.",
        "rightLabel": "class | PySpark",
        "displayText": "sql.types.DecimalType(precision=10, scale=0)",
        "text": "DecimalType()",
        "leftLabel": "sql.types.DecimalType"
    }, {
        "name": "sql.types.DecimalType.jsonValue",
        "type": "method",
        "leftLabel": "sql.types.DecimalType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.DecimalType.jsonValue()",
        "text": "jsonValue()"
    }, {
        "name": "sql.types.DecimalType.simpleString",
        "type": "method",
        "leftLabel": "sql.types.DecimalType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.DecimalType.simpleString()",
        "text": "simpleString()"
    }, {
        "name": "sql.types.DoubleType",
        "type": "class",
        "description": "PARAMETERS: \n             DESCRIPTION: Double data type, representing double precision floats.",
        "rightLabel": "class | PySpark",
        "displayText": "sql.types.DoubleType()",
        "text": "DoubleType()",
        "leftLabel": "sql.types.DoubleType"
    }, {
        "name": "sql.types.FloatType",
        "type": "class",
        "description": "PARAMETERS: \n             DESCRIPTION: Float data type, representing single precision floats.",
        "rightLabel": "class | PySpark",
        "displayText": "sql.types.FloatType()",
        "text": "FloatType()",
        "leftLabel": "sql.types.FloatType"
    }, {
        "name": "sql.types.ByteType",
        "type": "class",
        "description": "PARAMETERS: \n             DESCRIPTION: Byte data type, i.e. a signed integer in a single byte.",
        "rightLabel": "class | PySpark",
        "displayText": "sql.types.ByteType()",
        "text": "ByteType()",
        "leftLabel": "sql.types.ByteType"
    }, {
        "name": "sql.types.ByteType.simpleString",
        "type": "method",
        "leftLabel": "sql.types.ByteType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.ByteType.simpleString()",
        "text": "simpleString()"
    }, {
        "name": "sql.types.IntegerType",
        "type": "class",
        "description": "PARAMETERS: \n             DESCRIPTION: Int data type, i.e. a signed 32-bit integer.",
        "rightLabel": "class | PySpark",
        "displayText": "sql.types.IntegerType()",
        "text": "IntegerType()",
        "leftLabel": "sql.types.IntegerType"
    }, {
        "name": "sql.types.IntegerType.simpleString",
        "type": "method",
        "leftLabel": "sql.types.IntegerType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.IntegerType.simpleString()",
        "text": "simpleString()"
    }, {
        "name": "sql.types.LongType",
        "type": "class",
        "description": "PARAMETERS: \n             DESCRIPTION: Long data type, i.e. a signed 64-bit integer.",
        "rightLabel": "class | PySpark",
        "displayText": "sql.types.LongType()",
        "text": "LongType()",
        "leftLabel": "sql.types.LongType"
    }, {
        "name": "sql.types.LongType.simpleString",
        "type": "method",
        "leftLabel": "sql.types.LongType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.LongType.simpleString()",
        "text": "simpleString()"
    }, {
        "name": "sql.types.ShortType",
        "type": "class",
        "description": "PARAMETERS: \n             DESCRIPTION: Short data type, i.e. a signed 16-bit integer.",
        "rightLabel": "class | PySpark",
        "displayText": "sql.types.ShortType()",
        "text": "ShortType()",
        "leftLabel": "sql.types.ShortType"
    }, {
        "name": "sql.types.ShortType.simpleString",
        "type": "method",
        "leftLabel": "sql.types.ShortType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.ShortType.simpleString()",
        "text": "simpleString()"
    }, {
        "name": "sql.types.ArrayType",
        "type": "class",
        "description": "PARAMETERS: elementType, containsNull=True\n             DESCRIPTION: Array data type.",
        "rightLabel": "class | PySpark",
        "displayText": "sql.types.ArrayType(elementType, containsNull=True)",
        "text": "ArrayType()",
        "leftLabel": "sql.types.ArrayType"
    }, {
        "name": "sql.types.ArrayType.fromInternal",
        "type": "method",
        "leftLabel": "sql.types.ArrayType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.ArrayType.fromInternal(obj)",
        "text": "fromInternal()"
    }, {
        "name": "sql.types.ArrayType.jsonValue",
        "type": "method",
        "leftLabel": "sql.types.ArrayType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.ArrayType.jsonValue()",
        "text": "jsonValue()"
    }, {
        "name": "sql.types.ArrayType.needConversion",
        "type": "method",
        "leftLabel": "sql.types.ArrayType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.ArrayType.needConversion()",
        "text": "needConversion()"
    }, {
        "name": "sql.types.ArrayType.simpleString",
        "type": "method",
        "leftLabel": "sql.types.ArrayType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.ArrayType.simpleString()",
        "text": "simpleString()"
    }, {
        "name": "sql.types.ArrayType.toInternal",
        "type": "method",
        "leftLabel": "sql.types.ArrayType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.ArrayType.toInternal(obj)",
        "text": "toInternal()"
    }, {
        "name": "sql.types.MapType",
        "type": "class",
        "description": "PARAMETERS: keyType, valueType, valueContainsNull=True\n             DESCRIPTION: Map data type.",
        "rightLabel": "class | PySpark",
        "displayText": "sql.types.MapType(keyType, valueType, valueContainsNull=True)",
        "text": "MapType()",
        "leftLabel": "sql.types.MapType"
    }, {
        "name": "sql.types.MapType.fromInternal",
        "type": "method",
        "leftLabel": "sql.types.MapType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.MapType.fromInternal(obj)",
        "text": "fromInternal()"
    }, {
        "name": "sql.types.MapType.jsonValue",
        "type": "method",
        "leftLabel": "sql.types.MapType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.MapType.jsonValue()",
        "text": "jsonValue()"
    }, {
        "name": "sql.types.MapType.needConversion",
        "type": "method",
        "leftLabel": "sql.types.MapType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.MapType.needConversion()",
        "text": "needConversion()"
    }, {
        "name": "sql.types.MapType.simpleString",
        "type": "method",
        "leftLabel": "sql.types.MapType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.MapType.simpleString()",
        "text": "simpleString()"
    }, {
        "name": "sql.types.MapType.toInternal",
        "type": "method",
        "leftLabel": "sql.types.MapType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.MapType.toInternal(obj)",
        "text": "toInternal()"
    }, {
        "name": "sql.types.StructField",
        "type": "class",
        "description": "PARAMETERS: name, dataType, nullable=True, metadata=None\n             DESCRIPTION: A field in StructType.",
        "rightLabel": "class | PySpark",
        "displayText": "sql.types.StructField(name, dataType, nullable=True, metadata=None)",
        "text": "StructField()",
        "leftLabel": "sql.types.StructField"
    }, {
        "name": "sql.types.StructField.fromInternal",
        "type": "method",
        "leftLabel": "sql.types.StructField",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.StructField.fromInternal(obj)",
        "text": "fromInternal()"
    }, {
        "name": "sql.types.StructField.jsonValue",
        "type": "method",
        "leftLabel": "sql.types.StructField",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.StructField.jsonValue()",
        "text": "jsonValue()"
    }, {
        "name": "sql.types.StructField.needConversion",
        "type": "method",
        "leftLabel": "sql.types.StructField",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.StructField.needConversion()",
        "text": "needConversion()"
    }, {
        "name": "sql.types.StructField.simpleString",
        "type": "method",
        "leftLabel": "sql.types.StructField",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.StructField.simpleString()",
        "text": "simpleString()"
    }, {
        "name": "sql.types.StructField.toInternal",
        "type": "method",
        "leftLabel": "sql.types.StructField",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.StructField.toInternal(obj)",
        "text": "toInternal()"
    }, {
        "name": "sql.types.StructType",
        "type": "class",
        "description": "PARAMETERS: fields=None\n             DESCRIPTION: Struct type, consisting of a list of StructField.",
        "rightLabel": "class | PySpark",
        "displayText": "sql.types.StructType(fields=None)",
        "text": "StructType()",
        "leftLabel": "sql.types.StructType"
    }, {
        "name": "sql.types.StructType.add",
        "type": "method",
        "description": "PARAMETERS: field, data_type=None, nullable=True, metadata=None\n             DESCRIPTION: Construct a StructType by adding new elements to it to define the schema. The method acceptsundefinedeither:",
        "leftLabel": "sql.types.StructType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.StructType.add(field, data_type=None, nullable=True, metadata=None)",
        "text": "add()"
    }, {
        "name": "sql.types.StructType.fromInternal",
        "type": "method",
        "leftLabel": "sql.types.StructType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.StructType.fromInternal(obj)",
        "text": "fromInternal()"
    }, {
        "name": "sql.types.StructType.jsonValue",
        "type": "method",
        "leftLabel": "sql.types.StructType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.StructType.jsonValue()",
        "text": "jsonValue()"
    }, {
        "name": "sql.types.StructType.needConversion",
        "type": "method",
        "leftLabel": "sql.types.StructType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.StructType.needConversion()",
        "text": "needConversion()"
    }, {
        "name": "sql.types.StructType.simpleString",
        "type": "method",
        "leftLabel": "sql.types.StructType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.StructType.simpleString()",
        "text": "simpleString()"
    }, {
        "name": "sql.types.StructType.toInternal",
        "type": "method",
        "description": "PARAMETERS: obj\n             DESCRIPTION: A collections of builtin functions",
        "leftLabel": "sql.types.StructType",
        "rightLabel": "method | PySpark",
        "displayText": "sql.types.StructType.toInternal(obj)",
        "text": "toInternal()"
    }, {
        "name": "sql.functions.abs",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Computes the absolute value.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.abs(col)",
        "text": "abs()",
        "leftLabel": "sql.functions.abs"
    }, {
        "name": "sql.functions.acos",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Computes the cosine inverse of the given value; the returned angle is in the range0.0 through pi.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.acos(col)",
        "text": "acos()",
        "leftLabel": "sql.functions.acos"
    }, {
        "name": "sql.functions.add_months",
        "type": "function",
        "description": "PARAMETERS: start, months\n             DESCRIPTION: Returns the date that is months months after start",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.add_months(start, months)",
        "text": "add_months()",
        "leftLabel": "sql.functions.add_months"
    }, {
        "name": "sql.functions.approxCountDistinct",
        "type": "function",
        "description": "PARAMETERS: col, rsd=None\n             DESCRIPTION: Returns a new Column for approximate distinct count of col.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.approxCountDistinct(col, rsd=None)",
        "text": "approxCountDistinct()",
        "leftLabel": "sql.functions.approxCountDistinct"
    }, {
        "name": "sql.functions.array",
        "type": "function",
        "description": "PARAMETERS: *cols\n             DESCRIPTION: Creates a new array column.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.array(*cols)",
        "text": "array()",
        "leftLabel": "sql.functions.array"
    }, {
        "name": "sql.functions.array_contains",
        "type": "function",
        "description": "PARAMETERS: col, value\n             DESCRIPTION: Collection function: returns True if the array contains the given value. The collectionundefinedelements and value must be of the same type.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.array_contains(col, value)",
        "text": "array_contains()",
        "leftLabel": "sql.functions.array_contains"
    }, {
        "name": "sql.functions.asc",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Returns a sort expression based on the ascending order of the given column name.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.asc(col)",
        "text": "asc()",
        "leftLabel": "sql.functions.asc"
    }, {
        "name": "sql.functions.ascii",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Computes the numeric value of the first character of the string column.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.ascii(col)",
        "text": "ascii()",
        "leftLabel": "sql.functions.ascii"
    }, {
        "name": "sql.functions.asin",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Computes the sine inverse of the given value; the returned angle is in the range-pi/2 through pi/2.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.asin(col)",
        "text": "asin()",
        "leftLabel": "sql.functions.asin"
    }, {
        "name": "sql.functions.atan",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Computes the tangent inverse of the given value.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.atan(col)",
        "text": "atan()",
        "leftLabel": "sql.functions.atan"
    }, {
        "name": "sql.functions.atan2",
        "type": "function",
        "description": "PARAMETERS: col1, col2\n             DESCRIPTION: Returns the angle theta from the conversion of rectangular coordinates (x, y) topolar coordinates (r, theta).",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.atan2(col1, col2)",
        "text": "atan2()",
        "leftLabel": "sql.functions.atan2"
    }, {
        "name": "sql.functions.avg",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Aggregate function: returns the average of the values in a group.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.avg(col)",
        "text": "avg()",
        "leftLabel": "sql.functions.avg"
    }, {
        "name": "sql.functions.base64",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Computes the BASE64 encoding of a binary column and returns it as a string column.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.base64(col)",
        "text": "base64()",
        "leftLabel": "sql.functions.base64"
    }, {
        "name": "sql.functions.bin",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Returns the string representation of the binary value of the given column.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.bin(col)",
        "text": "bin()",
        "leftLabel": "sql.functions.bin"
    }, {
        "name": "sql.functions.bitwiseNOT",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Computes bitwise not.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.bitwiseNOT(col)",
        "text": "bitwiseNOT()",
        "leftLabel": "sql.functions.bitwiseNOT"
    }, {
        "name": "sql.functions.broadcast",
        "type": "function",
        "description": "PARAMETERS: df\n             DESCRIPTION: Marks a DataFrame as small enough for use in broadcast joins.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.broadcast(df)",
        "text": "broadcast()",
        "leftLabel": "sql.functions.broadcast"
    }, {
        "name": "sql.functions.cbrt",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Computes the cube-root of the given value.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.cbrt(col)",
        "text": "cbrt()",
        "leftLabel": "sql.functions.cbrt"
    }, {
        "name": "sql.functions.ceil",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Computes the ceiling of the given value.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.ceil(col)",
        "text": "ceil()",
        "leftLabel": "sql.functions.ceil"
    }, {
        "name": "sql.functions.coalesce",
        "type": "function",
        "description": "PARAMETERS: *cols\n             DESCRIPTION: Returns the first column that is not null.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.coalesce(*cols)",
        "text": "coalesce()",
        "leftLabel": "sql.functions.coalesce"
    }, {
        "name": "sql.functions.col",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Returns a Column based on the given column name.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.col(col)",
        "text": "col()",
        "leftLabel": "sql.functions.col"
    }, {
        "name": "sql.functions.collect_list",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Aggregate function: returns a list of objects with duplicates.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.collect_list(col)",
        "text": "collect_list()",
        "leftLabel": "sql.functions.collect_list"
    }, {
        "name": "sql.functions.collect_set",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Aggregate function: returns a set of objects with duplicate elements eliminated.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.collect_set(col)",
        "text": "collect_set()",
        "leftLabel": "sql.functions.collect_set"
    }, {
        "name": "sql.functions.column",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Returns a Column based on the given column name.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.column(col)",
        "text": "column()",
        "leftLabel": "sql.functions.column"
    }, {
        "name": "sql.functions.concat",
        "type": "function",
        "description": "PARAMETERS: *cols\n             DESCRIPTION: Concatenates multiple input string columns together into a single string column.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.concat(*cols)",
        "text": "concat()",
        "leftLabel": "sql.functions.concat"
    }, {
        "name": "sql.functions.concat_ws",
        "type": "function",
        "description": "PARAMETERS: sep, *cols\n             DESCRIPTION: Concatenates multiple input string columns together into a single string column,undefinedusing the given separator.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.concat_ws(sep, *cols)",
        "text": "concat_ws()",
        "leftLabel": "sql.functions.concat_ws"
    }, {
        "name": "sql.functions.conv",
        "type": "function",
        "description": "PARAMETERS: col, fromBase, toBase\n             DESCRIPTION: Convert a number in a string column from one base to another.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.conv(col, fromBase, toBase)",
        "text": "conv()",
        "leftLabel": "sql.functions.conv"
    }, {
        "name": "sql.functions.corr",
        "type": "function",
        "description": "PARAMETERS: col1, col2\n             DESCRIPTION: Returns a new Column for the Pearson Correlation Coefficient for col1undefinedand col2.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.corr(col1, col2)",
        "text": "corr()",
        "leftLabel": "sql.functions.corr"
    }, {
        "name": "sql.functions.cos",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Computes the cosine of the given value.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.cos(col)",
        "text": "cos()",
        "leftLabel": "sql.functions.cos"
    }, {
        "name": "sql.functions.cosh",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Computes the hyperbolic cosine of the given value.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.cosh(col)",
        "text": "cosh()",
        "leftLabel": "sql.functions.cosh"
    }, {
        "name": "sql.functions.count",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Aggregate function: returns the number of items in a group.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.count(col)",
        "text": "count()",
        "leftLabel": "sql.functions.count"
    }, {
        "name": "sql.functions.countDistinct",
        "type": "function",
        "description": "PARAMETERS: col, *cols\n             DESCRIPTION: Returns a new Column for distinct count of col or cols.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.countDistinct(col, *cols)",
        "text": "countDistinct()",
        "leftLabel": "sql.functions.countDistinct"
    }, {
        "name": "sql.functions.crc32",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Calculates the cyclic redundancy check value  (CRC32) of a binary column andundefinedreturns the value as a bigint.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.crc32(col)",
        "text": "crc32()",
        "leftLabel": "sql.functions.crc32"
    }, {
        "name": "sql.functions.cumeDist",
        "type": "function",
        "description": "PARAMETERS: \n             DESCRIPTION: Window function: .. note:: Deprecated in 1.6, use cume_dist instead.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.cumeDist()",
        "text": "cumeDist()",
        "leftLabel": "sql.functions.cumeDist"
    }, {
        "name": "sql.functions.cume_dist",
        "type": "function",
        "description": "PARAMETERS: \n             DESCRIPTION: Window function: returns the cumulative distribution of values within a window partition,undefinedi.e. the fraction of rows that are below the current row.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.cume_dist()",
        "text": "cume_dist()",
        "leftLabel": "sql.functions.cume_dist"
    }, {
        "name": "sql.functions.current_date",
        "type": "function",
        "description": "PARAMETERS: \n             DESCRIPTION: Returns the current date as a date column.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.current_date()",
        "text": "current_date()",
        "leftLabel": "sql.functions.current_date"
    }, {
        "name": "sql.functions.current_timestamp",
        "type": "function",
        "description": "PARAMETERS: \n             DESCRIPTION: Returns the current timestamp as a timestamp column.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.current_timestamp()",
        "text": "current_timestamp()",
        "leftLabel": "sql.functions.current_timestamp"
    }, {
        "name": "sql.functions.date_add",
        "type": "function",
        "description": "PARAMETERS: start, days\n             DESCRIPTION: Returns the date that is days days after start",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.date_add(start, days)",
        "text": "date_add()",
        "leftLabel": "sql.functions.date_add"
    }, {
        "name": "sql.functions.date_format",
        "type": "function",
        "description": "PARAMETERS: date, format\n             DESCRIPTION: Converts a date/timestamp/string to a value of string in the format specified by the dateundefinedformat given by the second argument.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.date_format(date, format)",
        "text": "date_format()",
        "leftLabel": "sql.functions.date_format"
    }, {
        "name": "sql.functions.date_sub",
        "type": "function",
        "description": "PARAMETERS: start, days\n             DESCRIPTION: Returns the date that is days days before start",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.date_sub(start, days)",
        "text": "date_sub()",
        "leftLabel": "sql.functions.date_sub"
    }, {
        "name": "sql.functions.datediff",
        "type": "function",
        "description": "PARAMETERS: end, start\n             DESCRIPTION: Returns the number of days from start to end.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.datediff(end, start)",
        "text": "datediff()",
        "leftLabel": "sql.functions.datediff"
    }, {
        "name": "sql.functions.dayofmonth",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Extract the day of the month of a given date as integer.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.dayofmonth(col)",
        "text": "dayofmonth()",
        "leftLabel": "sql.functions.dayofmonth"
    }, {
        "name": "sql.functions.dayofyear",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Extract the day of the year of a given date as integer.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.dayofyear(col)",
        "text": "dayofyear()",
        "leftLabel": "sql.functions.dayofyear"
    }, {
        "name": "sql.functions.decode",
        "type": "function",
        "description": "PARAMETERS: col, charset\n             DESCRIPTION: Computes the first argument into a string from a binary using the provided character setundefined(one of &#8216;US-ASCII&#8217;, &#8216;ISO-8859-1&#8217;, &#8216;UTF-8&#8217;, &#8216;UTF-16BE&#8217;, &#8216;UTF-16LE&#8217;, &#8216;UTF-16&#8217;).",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.decode(col, charset)",
        "text": "decode()",
        "leftLabel": "sql.functions.decode"
    }, {
        "name": "sql.functions.denseRank",
        "type": "function",
        "description": "PARAMETERS: \n             DESCRIPTION: Window function: .. note:: Deprecated in 1.6, use dense_rank instead.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.denseRank()",
        "text": "denseRank()",
        "leftLabel": "sql.functions.denseRank"
    }, {
        "name": "sql.functions.dense_rank",
        "type": "function",
        "description": "PARAMETERS: \n             DESCRIPTION: Window function: returns the rank of rows within a window partition, without any gaps.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.dense_rank()",
        "text": "dense_rank()",
        "leftLabel": "sql.functions.dense_rank"
    }, {
        "name": "sql.functions.desc",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Returns a sort expression based on the descending order of the given column name.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.desc(col)",
        "text": "desc()",
        "leftLabel": "sql.functions.desc"
    }, {
        "name": "sql.functions.encode",
        "type": "function",
        "description": "PARAMETERS: col, charset\n             DESCRIPTION: Computes the first argument into a binary from a string using the provided character setundefined(one of &#8216;US-ASCII&#8217;, &#8216;ISO-8859-1&#8217;, &#8216;UTF-8&#8217;, &#8216;UTF-16BE&#8217;, &#8216;UTF-16LE&#8217;, &#8216;UTF-16&#8217;).",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.encode(col, charset)",
        "text": "encode()",
        "leftLabel": "sql.functions.encode"
    }, {
        "name": "sql.functions.exp",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Computes the exponential of the given value.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.exp(col)",
        "text": "exp()",
        "leftLabel": "sql.functions.exp"
    }, {
        "name": "sql.functions.explode",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Returns a new row for each element in the given array or map.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.explode(col)",
        "text": "explode()",
        "leftLabel": "sql.functions.explode"
    }, {
        "name": "sql.functions.expm1",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Computes the exponential of the given value minus one.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.expm1(col)",
        "text": "expm1()",
        "leftLabel": "sql.functions.expm1"
    }, {
        "name": "sql.functions.expr",
        "type": "function",
        "description": "PARAMETERS: str\n             DESCRIPTION: Parses the expression string into the column that it represents",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.expr(str)",
        "text": "expr()",
        "leftLabel": "sql.functions.expr"
    }, {
        "name": "sql.functions.factorial",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Computes the factorial of the given value.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.factorial(col)",
        "text": "factorial()",
        "leftLabel": "sql.functions.factorial"
    }, {
        "name": "sql.functions.first",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Aggregate function: returns the first value in a group.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.first(col)",
        "text": "first()",
        "leftLabel": "sql.functions.first"
    }, {
        "name": "sql.functions.floor",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Computes the floor of the given value.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.floor(col)",
        "text": "floor()",
        "leftLabel": "sql.functions.floor"
    }, {
        "name": "sql.functions.format_number",
        "type": "function",
        "description": "PARAMETERS: col, d\n             DESCRIPTION: Formats the number X to a format like &#8216;#,&#8211;#,&#8211;#.&#8211;&#8217;, rounded to d decimal places,undefinedand returns the result as a string.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.format_number(col, d)",
        "text": "format_number()",
        "leftLabel": "sql.functions.format_number"
    }, {
        "name": "sql.functions.format_string",
        "type": "function",
        "description": "PARAMETERS: format, *cols\n             DESCRIPTION: Formats the arguments in printf-style and returns the result as a string column.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.format_string(format, *cols)",
        "text": "format_string()",
        "leftLabel": "sql.functions.format_string"
    }, {
        "name": "sql.functions.from_unixtime",
        "type": "function",
        "description": "PARAMETERS: timestamp, format='yyyy-MM-dd HH:mm:ss'\n             DESCRIPTION: Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a stringundefinedrepresenting the timestamp of that moment in the current system time zone in the given\nformat.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.from_unixtime(timestamp, format='yyyy-MM-dd HH:mm:ss')",
        "text": "from_unixtime()",
        "leftLabel": "sql.functions.from_unixtime"
    }, {
        "name": "sql.functions.from_utc_timestamp",
        "type": "function",
        "description": "PARAMETERS: timestamp, tz\n             DESCRIPTION: Assumes given timestamp is UTC and converts to given timezone.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.from_utc_timestamp(timestamp, tz)",
        "text": "from_utc_timestamp()",
        "leftLabel": "sql.functions.from_utc_timestamp"
    }, {
        "name": "sql.functions.get_json_object",
        "type": "function",
        "description": "PARAMETERS: col, path\n             DESCRIPTION: Extracts json object from a json string based on json path specified, and returns json stringundefinedof the extracted json object. It will return null if the input json string is invalid.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.get_json_object(col, path)",
        "text": "get_json_object()",
        "leftLabel": "sql.functions.get_json_object"
    }, {
        "name": "sql.functions.greatest",
        "type": "function",
        "description": "PARAMETERS: *cols\n             DESCRIPTION: Returns the greatest value of the list of column names, skipping null values.undefinedThis function takes at least 2 parameters. It will return null iff all parameters are null.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.greatest(*cols)",
        "text": "greatest()",
        "leftLabel": "sql.functions.greatest"
    }, {
        "name": "sql.functions.hex",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Computes hex value of the given column, which could be StringType,undefinedBinaryType, IntegerType or LongType.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.hex(col)",
        "text": "hex()",
        "leftLabel": "sql.functions.hex"
    }, {
        "name": "sql.functions.hour",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Extract the hours of a given date as integer.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.hour(col)",
        "text": "hour()",
        "leftLabel": "sql.functions.hour"
    }, {
        "name": "sql.functions.hypot",
        "type": "function",
        "description": "PARAMETERS: col1, col2\n             DESCRIPTION: Computes sqrt(a^2^ + b^2^) without intermediate overflow or underflow.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.hypot(col1, col2)",
        "text": "hypot()",
        "leftLabel": "sql.functions.hypot"
    }, {
        "name": "sql.functions.initcap",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Translate the first letter of each word to upper case in the sentence.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.initcap(col)",
        "text": "initcap()",
        "leftLabel": "sql.functions.initcap"
    }, {
        "name": "sql.functions.input_file_name",
        "type": "function",
        "description": "PARAMETERS: \n             DESCRIPTION: Creates a string column for the file name of the current Spark task.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.input_file_name()",
        "text": "input_file_name()",
        "leftLabel": "sql.functions.input_file_name"
    }, {
        "name": "sql.functions.instr",
        "type": "function",
        "description": "PARAMETERS: str, substr\n             DESCRIPTION: Locate the position of the first occurrence of substr column in the given string.undefinedReturns null if either of the arguments are null.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.instr(str, substr)",
        "text": "instr()",
        "leftLabel": "sql.functions.instr"
    }, {
        "name": "sql.functions.isnan",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: An expression that returns true iff the column is NaN.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.isnan(col)",
        "text": "isnan()",
        "leftLabel": "sql.functions.isnan"
    }, {
        "name": "sql.functions.isnull",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: An expression that returns true iff the column is null.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.isnull(col)",
        "text": "isnull()",
        "leftLabel": "sql.functions.isnull"
    }, {
        "name": "sql.functions.json_tuple",
        "type": "function",
        "description": "PARAMETERS: col, *fields\n             DESCRIPTION: Creates a new row for a json column according to the given field names.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.json_tuple(col, *fields)",
        "text": "json_tuple()",
        "leftLabel": "sql.functions.json_tuple"
    }, {
        "name": "sql.functions.kurtosis",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Aggregate function: returns the kurtosis of the values in a group.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.kurtosis(col)",
        "text": "kurtosis()",
        "leftLabel": "sql.functions.kurtosis"
    }, {
        "name": "sql.functions.lag",
        "type": "function",
        "description": "PARAMETERS: col, count=1, default=None\n             DESCRIPTION: Window function: returns the value that is offset rows before the current row, andundefineddefaultValue if there is less than offset rows before the current row. For example,\nan offset of one will return the previous row at any given point in the window partition.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.lag(col, count=1, default=None)",
        "text": "lag()",
        "leftLabel": "sql.functions.lag"
    }, {
        "name": "sql.functions.last",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Aggregate function: returns the last value in a group.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.last(col)",
        "text": "last()",
        "leftLabel": "sql.functions.last"
    }, {
        "name": "sql.functions.last_day",
        "type": "function",
        "description": "PARAMETERS: date\n             DESCRIPTION: Returns the last day of the month which the given date belongs to.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.last_day(date)",
        "text": "last_day()",
        "leftLabel": "sql.functions.last_day"
    }, {
        "name": "sql.functions.lead",
        "type": "function",
        "description": "PARAMETERS: col, count=1, default=None\n             DESCRIPTION: Window function: returns the value that is offset rows after the current row, andundefineddefaultValue if there is less than offset rows after the current row. For example,\nan offset of one will return the next row at any given point in the window partition.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.lead(col, count=1, default=None)",
        "text": "lead()",
        "leftLabel": "sql.functions.lead"
    }, {
        "name": "sql.functions.least",
        "type": "function",
        "description": "PARAMETERS: *cols\n             DESCRIPTION: Returns the least value of the list of column names, skipping null values.undefinedThis function takes at least 2 parameters. It will return null iff all parameters are null.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.least(*cols)",
        "text": "least()",
        "leftLabel": "sql.functions.least"
    }, {
        "name": "sql.functions.length",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Calculates the length of a string or binary expression.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.length(col)",
        "text": "length()",
        "leftLabel": "sql.functions.length"
    }, {
        "name": "sql.functions.levenshtein",
        "type": "function",
        "description": "PARAMETERS: left, right\n             DESCRIPTION: Computes the Levenshtein distance of the two given strings.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.levenshtein(left, right)",
        "text": "levenshtein()",
        "leftLabel": "sql.functions.levenshtein"
    }, {
        "name": "sql.functions.lit",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Creates a Column of literal value.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.lit(col)",
        "text": "lit()",
        "leftLabel": "sql.functions.lit"
    }, {
        "name": "sql.functions.locate",
        "type": "function",
        "description": "PARAMETERS: substr, str, pos=0\n             DESCRIPTION: Locate the position of the first occurrence of substr in a string column, after position pos.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.locate(substr, str, pos=0)",
        "text": "locate()",
        "leftLabel": "sql.functions.locate"
    }, {
        "name": "sql.functions.log",
        "type": "function",
        "description": "PARAMETERS: arg1, arg2=None\n             DESCRIPTION: Returns the first argument-based logarithm of the second argument.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.log(arg1, arg2=None)",
        "text": "log()",
        "leftLabel": "sql.functions.log"
    }, {
        "name": "sql.functions.log10",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Computes the logarithm of the given value in Base 10.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.log10(col)",
        "text": "log10()",
        "leftLabel": "sql.functions.log10"
    }, {
        "name": "sql.functions.log1p",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Computes the natural logarithm of the given value plus one.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.log1p(col)",
        "text": "log1p()",
        "leftLabel": "sql.functions.log1p"
    }, {
        "name": "sql.functions.log2",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Returns the base-2 logarithm of the argument.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.log2(col)",
        "text": "log2()",
        "leftLabel": "sql.functions.log2"
    }, {
        "name": "sql.functions.lower",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Converts a string column to lower case.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.lower(col)",
        "text": "lower()",
        "leftLabel": "sql.functions.lower"
    }, {
        "name": "sql.functions.lpad",
        "type": "function",
        "description": "PARAMETERS: col, len, pad\n             DESCRIPTION: Left-pad the string column to width len with pad.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.lpad(col, len, pad)",
        "text": "lpad()",
        "leftLabel": "sql.functions.lpad"
    }, {
        "name": "sql.functions.ltrim",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Trim the spaces from left end for the specified string value.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.ltrim(col)",
        "text": "ltrim()",
        "leftLabel": "sql.functions.ltrim"
    }, {
        "name": "sql.functions.max",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Aggregate function: returns the maximum value of the expression in a group.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.max(col)",
        "text": "max()",
        "leftLabel": "sql.functions.max"
    }, {
        "name": "sql.functions.md5",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Calculates the MD5 digest and returns the value as a 32 character hex string.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.md5(col)",
        "text": "md5()",
        "leftLabel": "sql.functions.md5"
    }, {
        "name": "sql.functions.mean",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Aggregate function: returns the average of the values in a group.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.mean(col)",
        "text": "mean()",
        "leftLabel": "sql.functions.mean"
    }, {
        "name": "sql.functions.min",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Aggregate function: returns the minimum value of the expression in a group.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.min(col)",
        "text": "min()",
        "leftLabel": "sql.functions.min"
    }, {
        "name": "sql.functions.minute",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Extract the minutes of a given date as integer.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.minute(col)",
        "text": "minute()",
        "leftLabel": "sql.functions.minute"
    }, {
        "name": "sql.functions.monotonicallyIncreasingId",
        "type": "function",
        "description": "PARAMETERS: \n             DESCRIPTION: New in version 1.4.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.monotonicallyIncreasingId()",
        "text": "monotonicallyIncreasingId()",
        "leftLabel": "sql.functions.monotonicallyIncreasingId"
    }, {
        "name": "sql.functions.monotonically_increasing_id",
        "type": "function",
        "description": "PARAMETERS: \n             DESCRIPTION: A column that generates monotonically increasing 64-bit integers.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.monotonically_increasing_id()",
        "text": "monotonically_increasing_id()",
        "leftLabel": "sql.functions.monotonically_increasing_id"
    }, {
        "name": "sql.functions.month",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Extract the month of a given date as integer.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.month(col)",
        "text": "month()",
        "leftLabel": "sql.functions.month"
    }, {
        "name": "sql.functions.months_between",
        "type": "function",
        "description": "PARAMETERS: date1, date2\n             DESCRIPTION: Returns the number of months between date1 and date2.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.months_between(date1, date2)",
        "text": "months_between()",
        "leftLabel": "sql.functions.months_between"
    }, {
        "name": "sql.functions.nanvl",
        "type": "function",
        "description": "PARAMETERS: col1, col2\n             DESCRIPTION: Returns col1 if it is not NaN, or col2 if col1 is NaN.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.nanvl(col1, col2)",
        "text": "nanvl()",
        "leftLabel": "sql.functions.nanvl"
    }, {
        "name": "sql.functions.next_day",
        "type": "function",
        "description": "PARAMETERS: date, dayOfWeek\n             DESCRIPTION: Returns the first date which is later than the value of the date column.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.next_day(date, dayOfWeek)",
        "text": "next_day()",
        "leftLabel": "sql.functions.next_day"
    }, {
        "name": "sql.functions.ntile",
        "type": "function",
        "description": "PARAMETERS: n\n             DESCRIPTION: Window function: returns the ntile group id (from 1 to n inclusive)undefinedin an ordered window partition. For example, if n is 4, the first\nquarter of the rows will get value 1, the second quarter will get 2,\nthe third quarter will get 3, and the last quarter will get 4.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.ntile(n)",
        "text": "ntile()",
        "leftLabel": "sql.functions.ntile"
    }, {
        "name": "sql.functions.percentRank",
        "type": "function",
        "description": "PARAMETERS: \n             DESCRIPTION: Window function: .. note:: Deprecated in 1.6, use percent_rank instead.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.percentRank()",
        "text": "percentRank()",
        "leftLabel": "sql.functions.percentRank"
    }, {
        "name": "sql.functions.percent_rank",
        "type": "function",
        "description": "PARAMETERS: \n             DESCRIPTION: Window function: returns the relative rank (i.e. percentile) of rows within a window partition.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.percent_rank()",
        "text": "percent_rank()",
        "leftLabel": "sql.functions.percent_rank"
    }, {
        "name": "sql.functions.pow",
        "type": "function",
        "description": "PARAMETERS: col1, col2\n             DESCRIPTION: Returns the value of the first argument raised to the power of the second argument.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.pow(col1, col2)",
        "text": "pow()",
        "leftLabel": "sql.functions.pow"
    }, {
        "name": "sql.functions.quarter",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Extract the quarter of a given date as integer.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.quarter(col)",
        "text": "quarter()",
        "leftLabel": "sql.functions.quarter"
    }, {
        "name": "sql.functions.rand",
        "type": "function",
        "description": "PARAMETERS: seed=None\n             DESCRIPTION: Generates a random column with i.i.d. samples from U[0.0, 1.0].",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.rand(seed=None)",
        "text": "rand()",
        "leftLabel": "sql.functions.rand"
    }, {
        "name": "sql.functions.randn",
        "type": "function",
        "description": "PARAMETERS: seed=None\n             DESCRIPTION: Generates a column with i.i.d. samples from the standard normal distribution.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.randn(seed=None)",
        "text": "randn()",
        "leftLabel": "sql.functions.randn"
    }, {
        "name": "sql.functions.rank",
        "type": "function",
        "description": "PARAMETERS: \n             DESCRIPTION: Window function: returns the rank of rows within a window partition.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.rank()",
        "text": "rank()",
        "leftLabel": "sql.functions.rank"
    }, {
        "name": "sql.functions.regexp_extract",
        "type": "function",
        "description": "PARAMETERS: str, pattern, idx\n             DESCRIPTION: Extract a specific(idx) group identified by a java regex, from the specified string column.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.regexp_extract(str, pattern, idx)",
        "text": "regexp_extract()",
        "leftLabel": "sql.functions.regexp_extract"
    }, {
        "name": "sql.functions.regexp_replace",
        "type": "function",
        "description": "PARAMETERS: str, pattern, replacement\n             DESCRIPTION: Replace all substrings of the specified string value that match regexp with rep.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.regexp_replace(str, pattern, replacement)",
        "text": "regexp_replace()",
        "leftLabel": "sql.functions.regexp_replace"
    }, {
        "name": "sql.functions.repeat",
        "type": "function",
        "description": "PARAMETERS: col, n\n             DESCRIPTION: Repeats a string column n times, and returns it as a new string column.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.repeat(col, n)",
        "text": "repeat()",
        "leftLabel": "sql.functions.repeat"
    }, {
        "name": "sql.functions.reverse",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Reverses the string column and returns it as a new string column.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.reverse(col)",
        "text": "reverse()",
        "leftLabel": "sql.functions.reverse"
    }, {
        "name": "sql.functions.rint",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Returns the double value that is closest in value to the argument and is equal to a mathematical integer.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.rint(col)",
        "text": "rint()",
        "leftLabel": "sql.functions.rint"
    }, {
        "name": "sql.functions.round",
        "type": "function",
        "description": "PARAMETERS: col, scale=0\n             DESCRIPTION: Round the value of e to scale decimal places if scale &gt;= 0undefinedor at integral part when scale &lt; 0.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.round(col, scale=0)",
        "text": "round()",
        "leftLabel": "sql.functions.round"
    }, {
        "name": "sql.functions.rowNumber",
        "type": "function",
        "description": "PARAMETERS: \n             DESCRIPTION: Window function: .. note:: Deprecated in 1.6, use row_number instead.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.rowNumber()",
        "text": "rowNumber()",
        "leftLabel": "sql.functions.rowNumber"
    }, {
        "name": "sql.functions.row_number",
        "type": "function",
        "description": "PARAMETERS: \n             DESCRIPTION: Window function: returns a sequential number starting at 1 within a window partition.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.row_number()",
        "text": "row_number()",
        "leftLabel": "sql.functions.row_number"
    }, {
        "name": "sql.functions.rpad",
        "type": "function",
        "description": "PARAMETERS: col, len, pad\n             DESCRIPTION: Right-pad the string column to width len with pad.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.rpad(col, len, pad)",
        "text": "rpad()",
        "leftLabel": "sql.functions.rpad"
    }, {
        "name": "sql.functions.rtrim",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Trim the spaces from right end for the specified string value.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.rtrim(col)",
        "text": "rtrim()",
        "leftLabel": "sql.functions.rtrim"
    }, {
        "name": "sql.functions.second",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Extract the seconds of a given date as integer.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.second(col)",
        "text": "second()",
        "leftLabel": "sql.functions.second"
    }, {
        "name": "sql.functions.sha1",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Returns the hex string result of SHA-1.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.sha1(col)",
        "text": "sha1()",
        "leftLabel": "sql.functions.sha1"
    }, {
        "name": "sql.functions.sha2",
        "type": "function",
        "description": "PARAMETERS: col, numBits\n             DESCRIPTION: Returns the hex string result of SHA-2 family of hash functions (SHA-224, SHA-256, SHA-384,undefinedand SHA-512). The numBits indicates the desired bit length of the result, which must have a\nvalue of 224, 256, 384, 512, or 0 (which is equivalent to 256).",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.sha2(col, numBits)",
        "text": "sha2()",
        "leftLabel": "sql.functions.sha2"
    }, {
        "name": "sql.functions.shiftLeft",
        "type": "function",
        "description": "PARAMETERS: col, numBits\n             DESCRIPTION: Shift the the given value numBits left.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.shiftLeft(col, numBits)",
        "text": "shiftLeft()",
        "leftLabel": "sql.functions.shiftLeft"
    }, {
        "name": "sql.functions.shiftRight",
        "type": "function",
        "description": "PARAMETERS: col, numBits\n             DESCRIPTION: Shift the the given value numBits right.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.shiftRight(col, numBits)",
        "text": "shiftRight()",
        "leftLabel": "sql.functions.shiftRight"
    }, {
        "name": "sql.functions.shiftRightUnsigned",
        "type": "function",
        "description": "PARAMETERS: col, numBits\n             DESCRIPTION: Unsigned shift the the given value numBits right.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.shiftRightUnsigned(col, numBits)",
        "text": "shiftRightUnsigned()",
        "leftLabel": "sql.functions.shiftRightUnsigned"
    }, {
        "name": "sql.functions.signum",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Computes the signum of the given value.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.signum(col)",
        "text": "signum()",
        "leftLabel": "sql.functions.signum"
    }, {
        "name": "sql.functions.sin",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Computes the sine of the given value.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.sin(col)",
        "text": "sin()",
        "leftLabel": "sql.functions.sin"
    }, {
        "name": "sql.functions.sinh",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Computes the hyperbolic sine of the given value.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.sinh(col)",
        "text": "sinh()",
        "leftLabel": "sql.functions.sinh"
    }, {
        "name": "sql.functions.size",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Collection function: returns the length of the array or map stored in the column.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.size(col)",
        "text": "size()",
        "leftLabel": "sql.functions.size"
    }, {
        "name": "sql.functions.skewness",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Aggregate function: returns the skewness of the values in a group.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.skewness(col)",
        "text": "skewness()",
        "leftLabel": "sql.functions.skewness"
    }, {
        "name": "sql.functions.sort_array",
        "type": "function",
        "description": "PARAMETERS: col, asc=True\n             DESCRIPTION: Collection function: sorts the input array for the given column in ascending order.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.sort_array(col, asc=True)",
        "text": "sort_array()",
        "leftLabel": "sql.functions.sort_array"
    }, {
        "name": "sql.functions.soundex",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Returns the SoundEx encoding for a string",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.soundex(col)",
        "text": "soundex()",
        "leftLabel": "sql.functions.soundex"
    }, {
        "name": "sql.functions.sparkPartitionId",
        "type": "function",
        "description": "PARAMETERS: \n             DESCRIPTION: New in version 1.4.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.sparkPartitionId()",
        "text": "sparkPartitionId()",
        "leftLabel": "sql.functions.sparkPartitionId"
    }, {
        "name": "sql.functions.spark_partition_id",
        "type": "function",
        "description": "PARAMETERS: \n             DESCRIPTION: A column for partition ID of the Spark task.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.spark_partition_id()",
        "text": "spark_partition_id()",
        "leftLabel": "sql.functions.spark_partition_id"
    }, {
        "name": "sql.functions.split",
        "type": "function",
        "description": "PARAMETERS: str, pattern\n             DESCRIPTION: Splits str around pattern (pattern is a regular expression).",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.split(str, pattern)",
        "text": "split()",
        "leftLabel": "sql.functions.split"
    }, {
        "name": "sql.functions.sqrt",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Computes the square root of the specified float value.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.sqrt(col)",
        "text": "sqrt()",
        "leftLabel": "sql.functions.sqrt"
    }, {
        "name": "sql.functions.stddev",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Aggregate function: returns the unbiased sample standard deviation of the expression in a group.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.stddev(col)",
        "text": "stddev()",
        "leftLabel": "sql.functions.stddev"
    }, {
        "name": "sql.functions.stddev_pop",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Aggregate function: returns population standard deviation of the expression in a group.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.stddev_pop(col)",
        "text": "stddev_pop()",
        "leftLabel": "sql.functions.stddev_pop"
    }, {
        "name": "sql.functions.stddev_samp",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Aggregate function: returns the unbiased sample standard deviation of the expression in a group.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.stddev_samp(col)",
        "text": "stddev_samp()",
        "leftLabel": "sql.functions.stddev_samp"
    }, {
        "name": "sql.functions.struct",
        "type": "function",
        "description": "PARAMETERS: *cols\n             DESCRIPTION: Creates a new struct column.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.struct(*cols)",
        "text": "struct()",
        "leftLabel": "sql.functions.struct"
    }, {
        "name": "sql.functions.substring",
        "type": "function",
        "description": "PARAMETERS: str, pos, len\n             DESCRIPTION: Substring starts at pos and is of length len when str is String type orundefinedreturns the slice of byte array that starts at pos in byte and is of length len\nwhen str is Binary type",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.substring(str, pos, len)",
        "text": "substring()",
        "leftLabel": "sql.functions.substring"
    }, {
        "name": "sql.functions.substring_index",
        "type": "function",
        "description": "PARAMETERS: str, delim, count\n             DESCRIPTION: Returns the substring from string str before count occurrences of the delimiter delim.undefinedIf count is positive, everything the left of the final delimiter (counting from left) is\nreturned. If count is negative, every to the right of the final delimiter (counting from the\nright) is returned. substring_index performs a case-sensitive match when searching for delim.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.substring_index(str, delim, count)",
        "text": "substring_index()",
        "leftLabel": "sql.functions.substring_index"
    }, {
        "name": "sql.functions.sum",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Aggregate function: returns the sum of all values in the expression.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.sum(col)",
        "text": "sum()",
        "leftLabel": "sql.functions.sum"
    }, {
        "name": "sql.functions.sumDistinct",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Aggregate function: returns the sum of distinct values in the expression.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.sumDistinct(col)",
        "text": "sumDistinct()",
        "leftLabel": "sql.functions.sumDistinct"
    }, {
        "name": "sql.functions.tan",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Computes the tangent of the given value.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.tan(col)",
        "text": "tan()",
        "leftLabel": "sql.functions.tan"
    }, {
        "name": "sql.functions.tanh",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Computes the hyperbolic tangent of the given value.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.tanh(col)",
        "text": "tanh()",
        "leftLabel": "sql.functions.tanh"
    }, {
        "name": "sql.functions.toDegrees",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Converts an angle measured in radians to an approximately equivalent angle measured in degrees.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.toDegrees(col)",
        "text": "toDegrees()",
        "leftLabel": "sql.functions.toDegrees"
    }, {
        "name": "sql.functions.toRadians",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Converts an angle measured in degrees to an approximately equivalent angle measured in radians.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.toRadians(col)",
        "text": "toRadians()",
        "leftLabel": "sql.functions.toRadians"
    }, {
        "name": "sql.functions.to_date",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Converts the column of StringType or TimestampType into DateType.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.to_date(col)",
        "text": "to_date()",
        "leftLabel": "sql.functions.to_date"
    }, {
        "name": "sql.functions.to_utc_timestamp",
        "type": "function",
        "description": "PARAMETERS: timestamp, tz\n             DESCRIPTION: Assumes given timestamp is in given timezone and converts to UTC.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.to_utc_timestamp(timestamp, tz)",
        "text": "to_utc_timestamp()",
        "leftLabel": "sql.functions.to_utc_timestamp"
    }, {
        "name": "sql.functions.translate",
        "type": "function",
        "description": "PARAMETERS: srcCol, matching, replace\n             DESCRIPTION: A function translate any character in the srcCol by a character in matching.undefinedThe characters in replace is corresponding to the characters in matching.\nThe translate will happen when any character in the string matching with the character\nin the matching.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.translate(srcCol, matching, replace)",
        "text": "translate()",
        "leftLabel": "sql.functions.translate"
    }, {
        "name": "sql.functions.trim",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Trim the spaces from both ends for the specified string column.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.trim(col)",
        "text": "trim()",
        "leftLabel": "sql.functions.trim"
    }, {
        "name": "sql.functions.trunc",
        "type": "function",
        "description": "PARAMETERS: date, format\n             DESCRIPTION: Returns date truncated to the unit specified by the format.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.trunc(date, format)",
        "text": "trunc()",
        "leftLabel": "sql.functions.trunc"
    }, {
        "name": "sql.functions.udf",
        "type": "function",
        "description": "PARAMETERS: f, returnType=StringType\n             DESCRIPTION: Creates a Column expression representing a user defined function (UDF).",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.udf(f, returnType=StringType)",
        "text": "udf()",
        "leftLabel": "sql.functions.udf"
    }, {
        "name": "sql.functions.unbase64",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Decodes a BASE64 encoded string column and returns it as a binary column.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.unbase64(col)",
        "text": "unbase64()",
        "leftLabel": "sql.functions.unbase64"
    }, {
        "name": "sql.functions.unhex",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Inverse of hex. Interprets each pair of characters as a hexadecimal numberundefinedand converts to the byte representation of number.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.unhex(col)",
        "text": "unhex()",
        "leftLabel": "sql.functions.unhex"
    }, {
        "name": "sql.functions.unix_timestamp",
        "type": "function",
        "description": "PARAMETERS: timestamp=None, format='yyyy-MM-dd HH:mm:ss'\n             DESCRIPTION: Convert time string with given pattern (&#8216;yyyy-MM-dd HH:mm:ss&#8217;, by default)undefinedto Unix time stamp (in seconds), using the default timezone and the default\nlocale, return null if fail.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.unix_timestamp(timestamp=None, format='yyyy-MM-dd HH:mm:ss')",
        "text": "unix_timestamp()",
        "leftLabel": "sql.functions.unix_timestamp"
    }, {
        "name": "sql.functions.upper",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Converts a string column to upper case.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.upper(col)",
        "text": "upper()",
        "leftLabel": "sql.functions.upper"
    }, {
        "name": "sql.functions.var_pop",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Aggregate function: returns the population variance of the values in a group.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.var_pop(col)",
        "text": "var_pop()",
        "leftLabel": "sql.functions.var_pop"
    }, {
        "name": "sql.functions.var_samp",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Aggregate function: returns the unbiased variance of the values in a group.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.var_samp(col)",
        "text": "var_samp()",
        "leftLabel": "sql.functions.var_samp"
    }, {
        "name": "sql.functions.variance",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Aggregate function: returns the population variance of the values in a group.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.variance(col)",
        "text": "variance()",
        "leftLabel": "sql.functions.variance"
    }, {
        "name": "sql.functions.weekofyear",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Extract the week number of a given date as integer.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.weekofyear(col)",
        "text": "weekofyear()",
        "leftLabel": "sql.functions.weekofyear"
    }, {
        "name": "sql.functions.when",
        "type": "function",
        "description": "PARAMETERS: condition, value\n             DESCRIPTION: Evaluates a list of conditions and returns one of multiple possible result expressions.undefinedIf Column.otherwise() is not invoked, None is returned for unmatched conditions.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.when(condition, value)",
        "text": "when()",
        "leftLabel": "sql.functions.when"
    }, {
        "name": "sql.functions.year",
        "type": "function",
        "description": "PARAMETERS: col\n             DESCRIPTION: Extract the year of a given date as integer.",
        "rightLabel": "function | PySpark",
        "displayText": "sql.functions.year(col)",
        "text": "year()",
        "leftLabel": "sql.functions.year"
    }]
}
